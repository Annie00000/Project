{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkortSLsFl8x7M3mcVp397",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annie00000/Project/blob/main/1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. one-way ANOVA"
      ],
      "metadata": {
        "id": "yIm3OM3xHcyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "def batch_one_way_anova(df, setting_dict):\n",
        "    \"\"\"\n",
        "    å°æŒ‡å®šçš„å¤šå€‹æ¬„ä½åˆ†åˆ¥é€²è¡Œ One-way ANOVA æª¢å®šã€‚\n",
        "\n",
        "    åƒæ•¸:\n",
        "    - df: pd.DataFrameï¼ŒåŸå§‹è³‡æ–™é›†\n",
        "    - setting_dict: åŒ…å« 'value_colname' (æ‡‰è®Šæ•¸) èˆ‡ 'factor_col_list' (è‡ªè®Šæ•¸æ¸…å–®) çš„å­—å…¸\n",
        "\n",
        "    å›å‚³:\n",
        "    - result_df: åŒ…å« Factor èˆ‡ P-value çš„ DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    target_y = setting_dict['value_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for factor in factors:\n",
        "        # æ ¹æ“š factor çš„ä¸åŒæ°´æº– (levels) é€²è¡Œåˆ†çµ„ï¼Œä¸¦æ”¶é›†å°æ‡‰çš„ y å€¼ (æå–æ¯å€‹ç¾¤çµ„ä¸­å°æ‡‰çš„æ•¸å€¼ï¼ˆæ‡‰è®Šæ•¸ï¼‰)\n",
        "        groups = [group[target_y].values for name, group in df.groupby(factor)]\n",
        "\n",
        "        # åŸ·è¡Œ One-way ANOVA\n",
        "        # *groups æœƒå°‡ list ä¸­çš„å„çµ„æ•¸æ“šæ‹†è§£ç‚º f_oneway çš„ç¨ç«‹åƒæ•¸\n",
        "        f_stat, p_val = stats.f_oneway(*groups)\n",
        "\n",
        "        results.append({\n",
        "            'Factor': factor,\n",
        "            'P-value': p_val\n",
        "        })\n",
        "\n",
        "    # è½‰æ›æˆ DataFrame æ ¼å¼è¼¸å‡º\n",
        "    result_df = pd.DataFrame(results)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# --- ä½¿ç”¨ç¯„ä¾‹ ---\n",
        "if __name__ == \"__main__\":\n",
        "    # å»ºç«‹ä¸€å€‹æ¸¬è©¦ç”¨çš„è³‡æ–™é›†\n",
        "    data = {\n",
        "        'Machine': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
        "        'Operator': ['Op1', 'Op2', 'Op1', 'Op2', 'Op1', 'Op2'],\n",
        "        'Yield': [89, 91, 70, 72, 85, 87]\n",
        "    }\n",
        "    test_df = pd.DataFrame(data)\n",
        "\n",
        "    settings = {\n",
        "        'value_colname': 'Yield',\n",
        "        'factor_col_list': ['Machine', 'Operator']\n",
        "    }\n",
        "\n",
        "    # åŸ·è¡Œå‡½å¼\n",
        "    anova_summary = batch_one_way_anova(test_df, settings)\n",
        "    print(anova_summary)"
      ],
      "metadata": {
        "id": "X-TJu8fou57g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ä¾‹å¤–è™•ç† (na or è©²æ¬„ä½åªæœ‰ä¸€ç¨®åˆ†é¡)"
      ],
      "metadata": {
        "id": "nkPLQuM70wTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def batch_one_way_anova_robust(df, setting_dict):\n",
        "    \"\"\"\n",
        "    é€²éšç‰ˆï¼šå°æŒ‡å®šæ¬„ä½é€²è¡Œ One-way ANOVAï¼ŒåŒ…å«éŒ¯èª¤è™•ç†èˆ‡æ•¸æ“šæª¢æŸ¥ã€‚\n",
        "    \"\"\"\n",
        "    target_y = setting_dict['value_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for factor in factors:\n",
        "        try:\n",
        "            # 1. é è™•ç†ï¼šæ’é™¤è©² Factor æˆ– Target Y ç‚ºç©ºå€¼çš„è³‡æ–™è¡Œ\n",
        "            clean_df = df[[factor, target_y]].dropna()\n",
        "\n",
        "            # 2. æª¢æŸ¥ï¼šè©²å› å­ä¸‹æ˜¯å¦æœ‰è‡³å°‘å…©å€‹é¡åˆ¥ (Levels)\n",
        "            groups_data = [group[target_y].values for name, group in clean_df.groupby(factor)]\n",
        "\n",
        "            if len(groups_data) < 2:\n",
        "                p_val = np.nan\n",
        "                remark = \"éŒ¯èª¤: å› å­é¡åˆ¥å°‘æ–¼ 2 é¡\"\n",
        "            else:\n",
        "                # 3. åŸ·è¡Œ ANOVA\n",
        "                f_stat, p_val = stats.f_oneway(*groups_data)\n",
        "                remark = \"æˆåŠŸ\"\n",
        "\n",
        "        except Exception as e:\n",
        "            # æ“·å–å…¶ä»–å¯èƒ½çš„éŒ¯èª¤ï¼ˆå¦‚ï¼šæ•¸æ“šå…¨ç‚ºå¸¸æ•¸ã€é¡å‹éŒ¯èª¤ç­‰ï¼‰\n",
        "            p_val = np.nan\n",
        "            remark = f\"ç™¼ç”ŸéŒ¯èª¤: {str(e)}\"\n",
        "\n",
        "        results.append({\n",
        "            'Factor': factor,\n",
        "            'P-value': p_val,\n",
        "            'Status': remark\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# --- æ¸¬è©¦ä¾‹å¤–ç‹€æ³ ---\n",
        "if __name__ == \"__main__\":\n",
        "    # å»ºç«‹åŒ…å«ç•°å¸¸æ•¸æ“šçš„ DataFrame\n",
        "    data = {\n",
        "        'Normal_Factor': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
        "        'One_Level_Factor': ['A', 'A', 'A', 'A', 'A', 'A'], # åªæœ‰ä¸€ç¨®åˆ†é¡\n",
        "        'Missing_Factor': ['A', 'A', 'B', None, 'C', 'C'],  # æœ‰ç©ºå€¼\n",
        "        'Yield': [89, 91, 70, 72, 85, 87]\n",
        "    }\n",
        "    test_df = pd.DataFrame(data)\n",
        "\n",
        "    settings = {\n",
        "        'value_colname': 'Yield',\n",
        "        'factor_col_list': ['Normal_Factor', 'One_Level_Factor', 'Missing_Factor']\n",
        "    }\n",
        "\n",
        "    anova_summary = batch_one_way_anova_robust(test_df, settings)\n",
        "    print(anova_summary)"
      ],
      "metadata": {
        "id": "aNVfZXOh02je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ä½¿ç”¨ 'å‘é‡åŒ–' åŠ å¿«é€Ÿåº¦ & ä¾‹å¤–è™•ç†"
      ],
      "metadata": {
        "id": "VkWgl6PM23Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- çµæ§‹åŒ–æª¢æŸ¥ï¼šåˆ©ç”¨ if/else é å…ˆæ’é™¤ã€Œåˆ†é¡ä¸è¶³ã€æˆ–ã€Œå…¨ç‚ºç©ºå€¼ã€çš„æƒ…æ³ã€‚\n",
        "\n",
        "- æ•¸å€¼ä¿è­·ï¼šé˜²æ­¢åˆ†æ¯ç‚º 0ï¼ˆä¾‹å¦‚æ‰€æœ‰æ•¸æ“šå®Œå…¨ä¸€æ¨£å°è‡´è®Šç•°æ•¸ç‚º 0ï¼‰é€ æˆç¨‹å¼å´©æ½°ã€‚"
      ],
      "metadata": {
        "id": "djx0Gu133AN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def fast_batch_anova_with_error_handling(df, setting_dict):\n",
        "    y_name = setting_dict['value_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "\n",
        "    # --- 1. å…¨åŸŸé è™•ç† ---\n",
        "    # ç¢ºä¿æ‡‰è®Šæ•¸ y æ˜¯æ•¸å€¼å‹æ…‹ä¸”ç§»é™¤ NaN (ANOVA å° y çš„ç¼ºå¤±å€¼å¾ˆæ•æ„Ÿ)\n",
        "    temp_df = df.copy()\n",
        "    temp_df[y_name] = pd.to_numeric(temp_df[y_name], errors='coerce')\n",
        "\n",
        "    # é€™è£¡åªé‡å° y åšåˆæ­¥éæ¿¾ï¼Œç¢ºä¿è¨ˆç®—ç¸½è®Šç•°æ™‚åŸºç¤ä¸€è‡´\n",
        "    valid_mask = temp_df[y_name].notna()\n",
        "    y = temp_df.loc[valid_mask, y_name].values\n",
        "\n",
        "    if len(y) == 0:\n",
        "        return pd.DataFrame(columns=['Factor', 'P-value', 'Status'])\n",
        "\n",
        "    n_total = len(y)\n",
        "    sum_y = np.sum(y)\n",
        "    ss_total = np.sum(y**2) - (sum_y**2) / n_total\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # --- 2. é€ä¸€å› å­é‹ç®— ---\n",
        "    for factor in factors:\n",
        "        try:\n",
        "            # å–å¾—ç•¶å‰å› å­çš„è³‡æ–™ (æ’é™¤è©²å› å­æœ¬èº«çš„ NaN)\n",
        "            f_series = temp_df.loc[valid_mask, factor]\n",
        "            current_y = y\n",
        "\n",
        "            # å¦‚æœå› å­æ¬„ä½æœ‰ NaNï¼Œéœ€äºŒæ¬¡éæ¿¾\n",
        "            if f_series.isna().any():\n",
        "                sub_mask = f_series.notna()\n",
        "                f_series = f_series[sub_mask]\n",
        "                current_y = y[sub_mask.values]\n",
        "\n",
        "            # æª¢æŸ¥æœ‰æ•ˆæ¨£æœ¬æ•¸\n",
        "            if len(f_series) < 2:\n",
        "                results.append({'Factor': factor, 'P-value': np.nan, 'Status': 'Error: æœ‰æ•ˆæ¨£æœ¬ä¸è¶³'})\n",
        "                continue\n",
        "\n",
        "            # ä½¿ç”¨ factorize åŠ é€Ÿåˆ†é¡\n",
        "            group_labels, obs = pd.factorize(f_series)\n",
        "            n_groups = len(obs)\n",
        "\n",
        "            if n_groups < 2:\n",
        "                results.append({'Factor': factor, 'P-value': np.nan, 'Status': 'Error: å› å­é¡åˆ¥ < 2'})\n",
        "                continue\n",
        "\n",
        "            # è¨ˆç®—çµ±è¨ˆé‡\n",
        "            counts = np.bincount(group_labels)\n",
        "            sums = np.bincount(group_labels, weights=current_y)\n",
        "\n",
        "            # é‡æ–°è¨ˆç®—è©²å› å­å°æ‡‰çš„ç¸½è®Šç•° (è‹¥æœ‰å›  NaN å‰”é™¤è³‡æ–™)\n",
        "            curr_n = len(current_y)\n",
        "            curr_ss_total = np.sum(current_y**2) - (np.sum(current_y)**2) / curr_n\n",
        "\n",
        "            ssb = np.sum(sums**2 / counts) - (np.sum(current_y)**2 / curr_n)\n",
        "            ssw = curr_ss_total - ssb\n",
        "\n",
        "            # é¿å…æ•¸å€¼ç²¾ç¢ºåº¦é€ æˆçš„å¾®å°è² æ•¸\n",
        "            ssw = max(ssw, 0)\n",
        "\n",
        "            df_between = n_groups - 1\n",
        "            df_within = curr_n - n_groups\n",
        "\n",
        "            if ssw == 0:\n",
        "                # çµ„å…§ç„¡è®Šç•°ï¼Œè‹¥çµ„é–“æœ‰è®Šç•°å‰‡ P å€¼æ¥µå°ï¼Œè‹¥çš†ç„¡å‰‡ç„¡æ³•è¨ˆç®—\n",
        "                p_val = 0.0 if ssb > 0 else np.nan\n",
        "                status = 'Success: çµ„å…§ç„¡è®Šç•°'\n",
        "            else:\n",
        "                f_stat = (ssb / df_between) / (ssw / df_within)\n",
        "                p_val = stats.f.sf(f_stat, df_between, df_within)\n",
        "                status = 'Success'\n",
        "\n",
        "            results.append({'Factor': factor, 'P-value': p_val, 'Status': status})\n",
        "\n",
        "        except Exception as e:\n",
        "            results.append({'Factor': factor, 'P-value': np.nan, 'Status': f'Unexpected Error: {str(e)}'})\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "GdiPw2Nv284m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**é€™å€‹ç‰ˆæœ¬å¼·åŒ–çš„ä¾‹å¤–è™•ç†ï¼š**\n",
        "1. è³‡æ–™é¡å‹å¼·åˆ¶è½‰æ›ï¼šä½¿ç”¨ pd.to_numeric(..., errors='coerce') ç¢ºä¿æ‡‰è®Šæ•¸ä¸€å®šæ˜¯æ•¸å­—ï¼Œéæ•¸å­—æœƒè®Šç‚º NaNã€‚\n",
        "\n",
        "2. é›™é‡ç¼ºå¤±å€¼éæ¿¾ï¼š\n",
        "  - å…ˆéæ¿¾ y çš„ NaNã€‚\n",
        "  - åœ¨è¿´åœˆå…§é‡å°æ¯å€‹ factor éæ¿¾å…¶ç‰¹æœ‰çš„ NaNï¼Œç¢ºä¿ bincount è¨ˆç®—æ™‚ç´¢å¼•èˆ‡æ•¸å€¼é•·åº¦å®Œå…¨åŒ¹é…ã€‚\n",
        "\n",
        "3. é‚Šç•Œæ•¸å€¼ä¿è­·ï¼š\n",
        "  -ssw = max(ssw, 0)ï¼šåœ¨æµ®é»æ•¸é‹ç®—ä¸­ï¼Œæ¥µå°çš„æ­£æ•¸æœ‰æ™‚æœƒè®Šæˆæ¥µå°çš„è² æ•¸ï¼ˆå¦‚ -1e-15ï¼‰ï¼Œé€™æœƒå°è‡´ F æª¢å®šå ±éŒ¯ï¼Œæ­¤è™•å¼·åˆ¶ä¿®æ­£ã€‚\n",
        "  - çµ„å…§è®Šç•°ç‚º 0 è™•ç†ï¼šç•¶ä¸€çµ„å…§æ•¸æ“šå®Œå…¨ç›¸åŒï¼ˆSSW=0ï¼‰ï¼Œé€™åœ¨è‡ªå‹•åŒ–åˆ†ææ•¸åƒå€‹æ¬„ä½æ™‚å¸¸ç™¼ç”Ÿï¼Œç›´æ¥çµ¦äºˆ P=0 (é¡¯è‘—) æˆ– NaNã€‚\n",
        "\n",
        "4. ç©©å®šæ€§ï¼šå³ä½¿å…¶ä¸­ä¸€å€‹å› å­å› ç‚ºè³‡æ–™æ¯€æå™´å‡º Unexpected Errorï¼Œè¿´åœˆä¹Ÿæœƒç¹¼çºŒåŸ·è¡Œä¸‹ä¸€å€‹å› å­ã€‚\n"
      ],
      "metadata": {
        "id": "GuIY_Zfg3pGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**é€™å€‹å„ªåŒ–ç‰ˆæœ¬ä¹‹æ‰€ä»¥å¿«ï¼ˆé€šå¸¸æ¯” stats.f_oneway å¿« 10 åˆ° 50 å€ï¼‰ï¼ŒåŸå› ä¸åœ¨æ–¼æ¶ˆæ»…äº†è¿´åœˆï¼Œè€Œåœ¨æ–¼ã€Œè¿´åœˆå…§åšäº†ä»€éº¼ã€ä»¥åŠã€Œé¿é–‹äº†å“ªäº›é–‹éŠ·ã€ã€‚**\n",
        "\n",
        "1. é¿é–‹äº† Python å‡½å¼å‘¼å«çš„ã€Œæ˜‚è²´ã€é–‹éŠ·:\n",
        "- stats.f_oneway(*groups) è¦æ±‚ä½ å‚³å…¥å¤šå€‹é™£åˆ—ã€‚åœ¨ for è¿´åœˆä¸­ï¼Œå¦‚æœä½ æœ‰ 1000 å€‹å› å­ï¼š\n",
        "\n",
        "  - f_oneway ç‰ˆæœ¬ï¼šæ¯ä¸€è¼ªéƒ½è¦é€²è¡Œ Python åˆ—è¡¨æ‹†è§£ (*groups)ã€å¤šé‡çš„è¼¸å…¥æª¢æŸ¥ï¼ˆæª¢æŸ¥æ˜¯å¦ç‚ºç©ºã€æ˜¯å¦ç‚ºæ•¸å€¼ï¼‰ã€è½‰æ›æˆ NumPy é™£åˆ—ã€‚\n",
        "\n",
        "  - å„ªåŒ–ç‰ˆæœ¬ï¼šç›´æ¥å‘¼å« NumPy çš„ C å¯¦ä½œåº•å±¤å‡½å¼ã€‚\n",
        "\n",
        "2. NumPy bincount çš„ç¥å¥‡æ•ˆèƒ½ï¼ˆæ ¸å¿ƒå·®ç•°ï¼‰:\n",
        "- é€™æ˜¯æœ€é—œéµçš„ä¸€é»ã€‚åœ¨æ¨™æº–åšæ³•ä¸­ï¼Œgroupby æœƒåœ¨ Python è¨˜æ†¶é«”ä¸­å»ºç«‹å¤šå€‹å°å‹ DataFrame ç‰©ä»¶ï¼Œé€™éå¸¸ç·©æ…¢ã€‚\n",
        "  - f_oneway æµç¨‹ï¼šgroupby -> åˆ†å‰²è³‡æ–™ -> å»ºç«‹å¤šå€‹ list/array -> å‚³å…¥å‡½å¼ã€‚\n",
        "\n",
        "  - å„ªåŒ–ç‰ˆæœ¬æµç¨‹ï¼špd.factorize å°‡æ¨™ç±¤è½‰ç‚ºæ•´æ•¸ï¼Œç„¶å¾Œ np.bincount ä¸€æ¬¡æ€§åœ¨ C åº•å±¤å®Œæˆåˆ†çµ„åŠ ç¸½ã€‚\n",
        "  \n",
        "  - np.bincount æ˜¯å°ˆé–€ç‚ºã€Œæ•´æ•¸ç´¢å¼•åŠ æ¬ŠåŠ ç¸½ã€è¨­è¨ˆçš„ï¼Œå®ƒä¸æœƒå»ºç«‹ä¸­é–“ç‰©ä»¶ï¼Œç›´æ¥åœ¨å…§å­˜ä¸­å®Œæˆè¨ˆç®—ï¼Œé€™æ¯” groupby().sum() å¿«éå¸¸å¤š\n",
        "\n",
        "3. è¨ˆç®—é‡çš„æ¥µç°¡åŒ–\n",
        "- stats.f_oneway æ˜¯ä¸€å€‹é€šç”¨å‡½å¼ï¼Œå®ƒç‚ºäº†æº–ç¢ºæ€§æœƒï¼š\n",
        "è¨ˆç®—æ¯çµ„çš„å‡å€¼ã€‚è¨ˆç®—æ¯çµ„çš„è®Šç•°æ•¸ã€‚é€²è¡Œå¤§é‡çš„è¼¸å…¥é©—è­‰ã€‚\n",
        "\n",
        "- ä½†åœ¨å„ªåŒ–ç‰ˆæœ¬ä¸­ï¼Œæˆ‘å€‘åˆ©ç”¨äº† ANOVA çš„å¹³æ–¹å’Œåˆ†è§£æ€§è³ªï¼š$$SS_{total} = SS_{between} + SS_{within}$$\n",
        "\n",
        "- æˆ‘å€‘é å…ˆç®—å¥½äº†å…¨åŸŸçš„ $SS_{total}$ï¼ˆé€™å°æ‰€æœ‰å› å­éƒ½ä¸€æ¨£ï¼Œåªéœ€è¦ç®—ä¸€æ¬¡ï¼ï¼‰ï¼Œåœ¨è¿´åœˆå…§åªéœ€è¦ç®—å‡º $SS_{between}$ï¼Œç„¶å¾Œç›´æ¥ç›¸æ¸›å¾—åˆ° $SS_{within}$ã€‚æˆ‘å€‘çœå»äº†ä¸€åŠä»¥ä¸Šçš„å¹³æ–¹é‹ç®—ã€‚"
      ],
      "metadata": {
        "id": "F-AGIZwA9wm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### å°å° æ¸¬è©¦\n",
        "import time\n",
        "\n",
        "# æ¸¬è©¦æ¨™æº–ç‰ˆ (stats.f_oneway)\n",
        "start = time.time()\n",
        "res1 = batch_one_way_anova(large_df, settings)\n",
        "print(f\"æ¨™æº–ç‰ˆè€—æ™‚: {time.time() - start:.4f}s\")\n",
        "\n",
        "# æ¸¬è©¦å„ªåŒ–ç‰ˆ (bincount)\n",
        "start = time.time()\n",
        "res2 = fast_batch_anova_with_error_handling(large_df, settings)\n",
        "print(f\"å„ªåŒ–ç‰ˆè€—æ™‚: {time.time() - start:.4f}s\")"
      ],
      "metadata": {
        "id": "HSPZnzDd-np8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. åŠ å¿«é‹ç®—é€Ÿåº¦ & ä¾‹å¤–è™•ç† & å¿«å–æ©Ÿåˆ¶ (Caching) (é‡è¤‡æ¬„ä½è¨ˆç®—åŒ–ç°¡)"
      ],
      "metadata": {
        "id": "JXdpWk1871z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def super_fast_anova(df, setting_dict):\n",
        "    y_name = setting_dict['value_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "\n",
        "    # é è™•ç†æ‡‰è®Šæ•¸ y\n",
        "    y_series = pd.to_numeric(df[y_name], errors='coerce')\n",
        "    valid_mask = y_series.notna()\n",
        "    y = y_series[valid_mask].values\n",
        "\n",
        "    if len(y) == 0:\n",
        "        return pd.DataFrame(columns=['Factor', 'P-value', 'Status'])\n",
        "\n",
        "    # é è¨ˆç®—ç¸½å¹³æ–¹å’Œ\n",
        "    n_total = len(y)\n",
        "    curr_sum_y = np.sum(y)\n",
        "    curr_ss_total = np.sum(y**2) - (curr_sum_y**2) / n_total\n",
        "\n",
        "    # å¿«å–å­—å…¸ï¼š {è³‡æ–™æŒ‡ç´‹: (P-value, Status)}\n",
        "    cache = {}\n",
        "    results = []\n",
        "\n",
        "    for factor in factors:\n",
        "        # å–å¾—è©²å› å­çš„è³‡æ–™ (å°é½Š y çš„æœ‰æ•ˆä½)\n",
        "        f_series = df.loc[valid_mask, factor]\n",
        "\n",
        "        # --- æ ¸å¿ƒå„ªåŒ–ï¼šç”¢ç”Ÿè³‡æ–™æŒ‡ç´‹ ---\n",
        "        # ä½¿ç”¨ tuple è½‰æ›ä½œç‚º keyï¼Œé€™èƒ½ä»£è¡¨è©²æ¬„ä½çš„ã€Œå…§å®¹çµ„åˆã€\n",
        "        # å¦‚æœæ¬„ä½å…§å®¹å®Œå…¨ä¸€æ¨£ï¼ŒæŒ‡ç´‹å°±æœƒä¸€æ¨£\n",
        "        data_fingerprint = tuple(f_series.values)\n",
        "\n",
        "        if data_fingerprint in cache:\n",
        "            p_val, status = cache[data_fingerprint]\n",
        "            results.append({'Factor': factor, 'P-value': p_val, 'Status': f'Cache Hit ({status})'})\n",
        "            continue\n",
        "\n",
        "        # --- è‹¥å¿«å–æœªå‘½ä¸­ï¼Œå‰‡åŸ·è¡Œé‹ç®— ---\n",
        "        try:\n",
        "            # è™•ç†å› å­å…§çš„ NaN\n",
        "            if f_series.isna().any():\n",
        "                # æ³¨æ„ï¼šè‹¥æœ‰ NaNï¼Œè³‡æ–™æŒ‡ç´‹æœƒä¸åŒï¼Œæ­¤è™•ç°¡åŒ–è™•ç†\n",
        "                # å¯¦å‹™ä¸Šå»ºè­°é å…ˆå¡«è£œæˆ–è™•ç† NaN ä»¥æ¥µå¤§åŒ–å¿«å–æ•ˆæœ\n",
        "                pass\n",
        "\n",
        "            group_labels, obs = pd.factorize(f_series)\n",
        "            n_groups = len(obs)\n",
        "\n",
        "            if n_groups < 2:\n",
        "                res = (np.nan, 'Error: Category < 2')\n",
        "            else:\n",
        "                counts = np.bincount(group_labels)\n",
        "                sums = np.bincount(group_labels, weights=y)\n",
        "\n",
        "                ssb = np.sum(sums**2 / counts) - (curr_sum_y**2 / n_total)\n",
        "                ssw = max(curr_ss_total - ssb, 0)\n",
        "\n",
        "                if ssw == 0:\n",
        "                    p_val = 0.0 if ssb > 0 else np.nan\n",
        "                    res = (p_val, 'Success (No Within-Var)')\n",
        "                else:\n",
        "                    f_stat = (ssb / (n_groups - 1)) / (ssw / (n_total - n_groups))\n",
        "                    p_val = stats.f.sf(f_stat, n_groups - 1, n_total - n_groups)\n",
        "                    res = (p_val, 'Success')\n",
        "\n",
        "        except Exception as e:\n",
        "            res = (np.nan, f'Error: {str(e)}')\n",
        "\n",
        "        # å­˜å…¥å¿«å–ä¸¦è¨˜éŒ„çµæœ\n",
        "        cache[data_fingerprint] = res\n",
        "        results.append({'Factor': factor, 'P-value': res[0], 'Status': res[1]})\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "iyygIRR2CUba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-1. ä¸Šæ–¹çš„å„ªåŒ–ç‰ˆæœ¬ :\n"
      ],
      "metadata": {
        "id": "CCvqCkpCDMcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "é å…ˆç¯©é¸å‡ºã€Œä¸é‡è¦†ã€çš„å› å­é€²è¡Œè¨ˆç®—ï¼Œæœ€å¾Œå†å°æ‡‰å›åŸæœ¬çš„æ¬„ä½ã€‚é€™æ¯”å¤šé€²ç¨‹æ›´ç©©å®šä¸”ä¸éœ€è¦é¡å¤–è³‡æºã€‚\n",
        "\n",
        "- ã€Œçµ‚æ¥µå„ªåŒ–ç‰ˆã€ï¼šçµåˆäº†å”¯ä¸€æ€§ç¯©é¸èˆ‡ NumPy å‘é‡åŒ–ã€‚"
      ],
      "metadata": {
        "id": "RMw3nZl3DW-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def ultimate_anova_processor(df, setting_dict):\n",
        "    y_name = setting_dict['value_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "\n",
        "    # 1. é è™•ç†æ‡‰è®Šæ•¸\n",
        "    y = pd.to_numeric(df[y_name], errors='coerce').values\n",
        "    valid_mask = ~np.isnan(y)\n",
        "    y_clean = y[valid_mask]\n",
        "    n_total = len(y_clean)\n",
        "\n",
        "    if n_total < 2: return pd.DataFrame()\n",
        "\n",
        "    # é è¨ˆç®—å…¨åŸŸçµ±è¨ˆé‡\n",
        "    sum_y = np.sum(y_clean)\n",
        "    ss_total = np.sum(y_clean**2) - (sum_y**2) / n_total\n",
        "\n",
        "    # 2. æ‰¾å‡ºã€Œå…§å®¹å”¯ä¸€ã€çš„æ¬„ä½çµ„åˆï¼Œé¿å…é‡è¤‡è¨ˆç®—\n",
        "    # æˆ‘å€‘åªå°ä¸åŒçš„å…§å®¹é€²è¡Œä¸€æ¬¡ ANOVA\n",
        "    unique_patterns = {} # å­˜æ”¾ {tuple_data: first_factor_name}\n",
        "    mapping = {}         # å­˜æ”¾ {factor_name: tuple_data}\n",
        "\n",
        "    for f in factors:\n",
        "        # è½‰æˆ tuple ä½œç‚º Key\n",
        "        pattern = tuple(df.loc[valid_mask, f].fillna(\"NAN_VAL\").values)\n",
        "        mapping[f] = pattern\n",
        "        if pattern not in unique_patterns:\n",
        "            unique_patterns[pattern] = f\n",
        "\n",
        "    # 3. åƒ…å°å”¯ä¸€æ¨¡å¼é€²è¡Œè¨ˆç®—\n",
        "    distinct_results = {}\n",
        "    for pattern, first_f_name in unique_patterns.items():\n",
        "        try:\n",
        "            # ä½¿ç”¨ pd.factorize çš„ C å¯¦ä½œåŠ é€Ÿ\n",
        "            labels, uniques = pd.factorize(pattern)\n",
        "            n_groups = len(uniques)\n",
        "\n",
        "            if n_groups < 2:\n",
        "                distinct_results[pattern] = (np.nan, \"Category < 2\")\n",
        "                continue\n",
        "\n",
        "            counts = np.bincount(labels)\n",
        "            sums = np.bincount(labels, weights=y_clean)\n",
        "\n",
        "            ssb = np.sum(sums**2 / counts) - (sum_y**2 / n_total)\n",
        "            ssw = max(ss_total - ssb, 0)\n",
        "\n",
        "            if ssw == 0:\n",
        "                p = 0.0 if ssb > 0 else np.nan\n",
        "            else:\n",
        "                f_stat = (ssb / (n_groups - 1)) / (ssw / (n_total - n_groups))\n",
        "                p = stats.f.sf(f_stat, n_groups - 1, n_total - n_groups)\n",
        "\n",
        "            distinct_results[pattern] = (p, \"Success\")\n",
        "        except:\n",
        "            distinct_results[pattern] = (np.nan, \"Error\")\n",
        "\n",
        "    # 4. å°‡çµæœæ˜ å°„å›åŸå§‹çš„æ‰€æœ‰æ¬„ä½\n",
        "    final_output = []\n",
        "    for f in factors:\n",
        "        p_val, status = distinct_results[mapping[f]]\n",
        "        final_output.append({'Factor': f, 'P-value': p_val, 'Status': status})\n",
        "\n",
        "    return pd.DataFrame(final_output)"
      ],
      "metadata": {
        "id": "lhCCmsb4DR4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. é«˜æ€§èƒ½å¯¦ä½œç‰ˆæœ¬ (Hybrid Adaptive ANOVA)\n",
        "é€™å€‹ç‰ˆæœ¬æœƒæ ¹æ“šè³‡æ–™é‡è‡ªå‹•æ±ºå®šæ˜¯å¦å•Ÿå‹•å¹³è¡Œé‹ç®—ï¼Œä¸¦å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨"
      ],
      "metadata": {
        "id": "RiIJeCUJJLy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "å°æ–¼ æ¥µå°åˆ°æ¥µå¤§çš„è®Šå‹•å€é–“ï¼Œå»ºè­°æ¡ç”¨ ã€Œæ‰¹æ¬¡è™•ç† (Batching) + æ··åˆç­–ç•¥ã€ï¼š\n",
        "\n",
        "- è‡ªå‹•åˆ¤å®šï¼šåˆ¤æ–· Rows * Cols çš„ç¸½é‡ã€‚å¦‚æœè¶…éé–€æª»ï¼ˆä¾‹å¦‚ 100 è¬å€‹æ•¸æ“šé»ï¼‰ï¼Œæ‰å•Ÿå‹•å¤šé€²ç¨‹ã€‚\n",
        "\n",
        "- é å…ˆç°¡åŒ–ï¼šä¸è«–æ•¸æ“šå¤§å°ï¼Œå…ˆç”¨ pd.factorize å’Œ mapping å‰”é™¤é‡è¤‡æ¬„ä½ã€‚\n",
        "\n",
        "- åˆ†æ‰¹ (Chunks)ï¼šä¸è¦æŠŠ 1,000 å€‹æ¬„ä½ä¸€æ¬¡å…¨å¡é€²å¤šé€²ç¨‹ï¼Œè€Œæ˜¯åˆ†æ‰¹è™•ç†ï¼Œå¹³è¡¡è² æ“”"
      ],
      "metadata": {
        "id": "40bNhlD0JU0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "def single_anova_calc(args):\n",
        "    \"\"\"å­é€²ç¨‹å°ˆç”¨çš„æœ€å°é‹ç®—å–®å…ƒ\"\"\"\n",
        "    pattern, y_clean, n_total, sum_y, ss_total = args\n",
        "    try:\n",
        "        labels, uniques = pd.factorize(pattern)\n",
        "        n_groups = len(uniques)\n",
        "        if n_groups < 2: return (np.nan, \"Category < 2\")\n",
        "\n",
        "        counts = np.bincount(labels)\n",
        "        sums = np.bincount(labels, weights=y_clean)\n",
        "\n",
        "        ssb = np.sum(sums**2 / counts) - (sum_y**2 / n_total)\n",
        "        ssw = max(ss_total - ssb, 0)\n",
        "\n",
        "        if ssw == 0:\n",
        "            p = 0.0 if ssb > 0 else np.nan\n",
        "            status = \"Success (No Within-Var)\"\n",
        "        else:\n",
        "            f_stat = (ssb / (n_groups - 1)) / (ssw / (n_total - n_groups))\n",
        "            p = stats.f.sf(f_stat, n_groups - 1, n_total - n_groups)\n",
        "            status = \"Success\"\n",
        "        return (p, status)\n",
        "    except:\n",
        "        return (np.nan, \"Error\")\n",
        "\n",
        "def robust_parallel_anova(df, setting_dict):\n",
        "    y_name = setting_dict['value_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "\n",
        "    # 1. é è™•ç†æ‡‰è®Šæ•¸ (ä¸€æ¬¡æ€§)\n",
        "    y = pd.to_numeric(df[y_name], errors='coerce').values\n",
        "    valid_mask = ~np.isnan(y)\n",
        "    y_clean = y[valid_mask]\n",
        "    n_total = len(y_clean)\n",
        "    sum_y = np.sum(y_clean)\n",
        "    ss_total = np.sum(y_clean**2) - (sum_y**2) / n_total\n",
        "\n",
        "    # 2. é›œæ¹Šèˆ‡å»é‡ (ä¸è«–è³‡æ–™å¤§å°ï¼Œé€™æ­¥å° 1000 cols éƒ½å¾ˆæœ‰ç”¨)\n",
        "    unique_patterns = {}\n",
        "    mapping = {}\n",
        "    for f in factors:\n",
        "        # é‡å°å¤§æ•¸æ“šï¼Œä½¿ç”¨ hash è™•ç† tuple ä»¥ç¯€çœè¨˜æ†¶é«”\n",
        "        pattern = tuple(df.loc[valid_mask, f].fillna(\"NAN\").values)\n",
        "        mapping[f] = pattern\n",
        "        if pattern not in unique_patterns:\n",
        "            unique_patterns[pattern] = f\n",
        "\n",
        "    # 3. ç­–ç•¥æ±ºç­–ï¼šåˆ¤æ–·æ˜¯å¦éœ€è¦å¹³è¡Œé‹ç®—\n",
        "    # é–€æª»å€¼ï¼š(æ¨¡å¼æ•¸é‡ * è³‡æ–™ç­†æ•¸) > 5,000,000\n",
        "    task_load = len(unique_patterns) * n_total\n",
        "    distinct_patterns = list(unique_patterns.keys())\n",
        "\n",
        "    if task_load > 5_000_000:\n",
        "        # --- å¤šé€²ç¨‹æ¨¡å¼ ---\n",
        "        # å°è£åƒæ•¸\n",
        "        args_list = [(p, y_clean, n_total, sum_y, ss_total) for p in distinct_patterns]\n",
        "        with Pool(processes=cpu_count()) as pool:\n",
        "            raw_results = pool.map(single_anova_calc, args_list)\n",
        "        distinct_results = dict(zip(distinct_patterns, raw_results))\n",
        "    else:\n",
        "        # --- å–®é€²ç¨‹æ¨¡å¼ ---\n",
        "        distinct_results = {p: single_anova_calc((p, y_clean, n_total, sum_y, ss_total))\n",
        "                           for p in distinct_patterns}\n",
        "\n",
        "    # 4. å°æ‡‰å›åŸå§‹æ¬„ä½\n",
        "    final_output = []\n",
        "    for f in factors:\n",
        "        p_val, status = distinct_results[mapping[f]]\n",
        "        final_output.append({'Factor': f, 'P-value': p_val, 'Status': status})\n",
        "\n",
        "    return pd.DataFrame(final_output)"
      ],
      "metadata": {
        "id": "GF3W-69zJNLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. ANOVA æ‰¹æ¬¡æª¢å®šç³»çµ±ç¸½çµ"
      ],
      "metadata": {
        "id": "aARf4JPsLTgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. æ ¸å¿ƒæ¼”ç®—æ³•ï¼šå‘é‡åŒ–é‹ç®— (Vectorization)**\n",
        "  - æ¨æ£„å‚³çµ±æ–¹æ³•ï¼šå‚³çµ± groupby æ­é… stats.f_oneway æœƒç”¢ç”Ÿå¤§é‡ Python ç‰©ä»¶é–‹éŠ·ï¼Œåœ¨è™•ç†æ•¸ç™¾å€‹æ¬„ä½æ™‚æ¥µæ…¢ã€‚\n",
        "  - æ¡ç”¨çŸ©é™£é‚è¼¯ï¼šåˆ©ç”¨ NumPy çš„ bincount ç›´æ¥åœ¨åº•å±¤ C èªè¨€å±¤ç´šé€²è¡Œåˆ†çµ„æ±‚å’Œï¼Œå°‡è¨ˆç®—è¤‡é›œåº¦å¾ $O(N \\times M)$ é™è‡³æ¥µä½ï¼Œæ•ˆèƒ½æå‡å¯é” 20-50 å€ã€‚\n",
        "  - å¹³æ–¹å’Œåˆ†è§£ï¼šé è¨ˆç®—å…¨åŸŸç¸½å¹³æ–¹å’Œ ($SS_{total}$)ï¼Œè¿´åœˆå…§åƒ…éœ€è¨ˆç®—çµ„é–“è®Šç•° ($SSB$)ï¼Œç¯€çœä¸€åŠä»¥ä¸Šçš„ç®—åŠ›ã€‚\n",
        "\n",
        "**2. æ™ºæ…§å¿«å–æ©Ÿåˆ¶ (Caching & Hashing)**\n",
        "  - å…§å®¹æŒ‡ç´‹è­˜åˆ¥ï¼šè‹¥å¤šå€‹æ¬„ä½çš„åˆ†é¡æ•¸æ“šå®Œå…¨ç›¸åŒï¼ˆä¾‹å¦‚ä¸åŒåç¨±ä½†å…§å®¹ä¸€è‡´çš„ IDï¼‰ï¼Œç³»çµ±æœƒé€é tuple é›œæ¹ŠæŠ€è¡“è­˜åˆ¥ã€‚\n",
        "  - é¿å…å†—é¤˜è¨ˆç®—ï¼šå°ç›¸åŒçš„åˆ†é¡æ¨¡å¼åƒ…è¨ˆç®—ä¸€æ¬¡ ANOVAï¼Œçµæœç›´æ¥å­˜å…¥å¿«å–ä¾›å¾ŒçºŒé‡è¤‡èª¿ç”¨ï¼Œæ¥µå¤§åŒ–æå‡è™•ç† 1000+ Cols æ™‚çš„æ•ˆç‡ã€‚\n",
        "\n",
        "**3. é«˜ç©©å®šæ€§çš„ä¾‹å¤–è™•ç† (Robustness)**\n",
        "  - å…©éšæ®µéæ¿¾ï¼šè‡ªå‹•è™•ç†æ‡‰è®Šæ•¸ ($y$) èˆ‡è‡ªè®Šæ•¸ ($Factor$) çš„ç¼ºå¤±å€¼ ($NaN$)ã€‚\n",
        "  - é‚Šç•Œæ¢ä»¶ä¿è­·ï¼š\n",
        "    - è‡ªå‹•åµæ¸¬åˆ†é¡å°‘æ–¼ 2 é¡çš„æƒ…æ³ã€‚\n",
        "    - è™•ç†ã€Œçµ„å…§ç„¡è®Šç•°ï¼ˆSSW=0ï¼‰ã€çš„æƒ…æ³ï¼Œé˜²æ­¢ F æª¢å®šåˆ†æ¯ç‚ºé›¶å°è‡´ç¨‹å¼å´©æ½°ã€‚\n",
        "  - éŒ¯èª¤è¿½è¹¤ï¼šè¼¸å‡ºçµæœåŒ…å« Status æ¬„ä½ï¼Œèƒ½æ˜ç¢ºæ¨™è¨»æ¯å€‹å› å­è¨ˆç®—æˆåŠŸæˆ–å¤±æ•—çš„åŸå› ã€‚\n",
        "\n",
        "**4. é©æ‡‰æ€§å¹³è¡Œè™•ç† (Adaptive Multiprocessing)**\n",
        "  - å‹•æ…‹é–€æª»åˆ‡æ›ï¼šç³»çµ±æœƒè‡ªå‹•è©•ä¼°ã€Œæ•¸æ“šè¦æ¨¡ï¼ˆ$Rows \\times Cols$ï¼‰ã€ã€‚\n",
        "    - å°æ•¸æ“šï¼šç¶­æŒå–®åŸ·è¡Œç·’ï¼Œé¿å…é€²ç¨‹å•Ÿå‹•èˆ‡è³‡æ–™åºåˆ—åŒ–ï¼ˆOverheadï¼‰é€ æˆçš„é€Ÿåº¦è¡°æ¸›ã€‚\n",
        "    - å¤§æ•¸æ“šï¼ˆå¦‚ $200,000$ Rowsï¼‰ï¼šå•Ÿå‹• multiprocessing.Poolï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸ CPUã€‚\n",
        "  - è³‡æºç®¡ç†ï¼šæ¡ç”¨ with ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¢ºä¿å­é€²ç¨‹çµæŸå¾Œè‡ªå‹•å›æ”¶ï¼Œä¸¦é€é chunksize å„ªåŒ–é€šè¨Šæ•ˆç‡ã€‚\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xFdSTJplLXnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-1. ç°¡åŒ–ç‰ˆæœ¬"
      ],
      "metadata": {
        "id": "4DhtHQ5mMcj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸš€ é«˜æ•ˆèƒ½ ANOVA è‡ªå‹•åŒ–æª¢å®šç³»çµ±\n",
        "\n",
        "\n",
        "**1. æ¥µé€Ÿé‹ç®— (Fast Calculation)**\n",
        "- å‘é‡åŒ–å–ä»£è¿´åœˆï¼šä½¿ç”¨ NumPy bincount é€²è¡Œåº•å±¤é‹ç®—ï¼Œé¿é–‹ç¹é‡çš„ groupby ç‰©ä»¶å»ºç«‹ã€‚\n",
        "\n",
        "- æ•ˆèƒ½è¡¨ç¾ï¼š300 æ¬„ä½ã€1000 åˆ—æ•¸æ“šåƒ…éœ€ < 0.1 ç§’ï¼Œæ¯”æ¨™æº–åšæ³•å¿« 20 å€ä»¥ä¸Šã€‚\n",
        "\n",
        "- é è¨ˆç®—æŠ€è¡“ï¼šç¸½è®Šç•°é‡åªç®—ä¸€æ¬¡ï¼Œå¤§å¹…æ¸›å°‘é‡è¤‡çš„å¹³æ–¹é‹ç®—ã€‚\n",
        "\n",
        "**2. æ™ºæ…§å»é‡ (Smart Redundancy Removal)**\n",
        "- æŒ‡ç´‹è¾¨è­˜ (Hashing)ï¼šè‡ªå‹•è­˜åˆ¥å…§å®¹å®Œå…¨ç›¸åŒçš„æ¬„ä½ï¼ˆå³ä¾¿åç¨±ä¸åŒï¼‰ã€‚\n",
        "\n",
        "- çµæœå¿«å– (Caching)ï¼šåŒæ¨¡å¼åƒ…è¨ˆç®—ä¸€æ¬¡ï¼Œç›´æ¥æ˜ å°„çµæœï¼Œè™•ç†åƒå€‹æ¬„ä½æ™‚æ¥µå…·å„ªå‹¢ã€‚\n",
        "\n",
        "**3. é©æ‡‰æ€§è¦æ¨¡ (Adaptive Scaling)**\n",
        "- å‹•æ…‹å¹³è¡Œè™•ç†ï¼š\n",
        "\n",
        "  - å°æ•¸æ“šï¼šå–®ç·šç¨‹åŸ·è¡Œï¼ˆçœå»é€²ç¨‹å‚³è¼¸é–‹éŠ·ï¼‰ã€‚\n",
        "\n",
        "  - å¤§æ•¸æ“š (å¦‚ 20è¬åˆ—)ï¼šè‡ªå‹•å•Ÿå‹• Multiprocessing å¤šæ ¸ä¸¦è¡Œã€‚\n",
        "\n",
        "-åˆ†å¡Šå‚³è¼¸ï¼šå„ªåŒ– chunksizeï¼Œç¢ºä¿å¤§æ•¸æ“šå‚³è¼¸æ™‚ä¸å¡è»Šã€‚\n",
        "\n",
        "**4. å·¥æ¥­ç´šç©©å®š (Industrial Stability)**\n",
        "- è‡ªå‹•å®¹éŒ¯ï¼šå…§å»ºè™•ç† NaN ç¼ºå¤±å€¼ã€åˆ†é¡æ•¸ä¸è¶³ã€çµ„å…§ç„¡è®Šç•°ï¼ˆSSW=0ï¼‰ç­‰ç•°å¸¸ã€‚\n",
        "\n",
        "- ç‹€æ…‹é€æ˜ï¼šæ¯å€‹çµæœæ¨™è¨»åŸ·è¡Œç‹€æ…‹ï¼ˆSuccess / Errorï¼‰ï¼Œç¢ºä¿å ±å‘Šæ•¸æ“šä¾†æºå¯é ã€‚"
      ],
      "metadata": {
        "id": "c1rB4T4pMfiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-2. ğŸ’¡ æœƒè­° Q&A æŠ€è¡“å°æŠ„"
      ],
      "metadata": {
        "id": "j4WqXLxfNKqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Q1ï¼šç‚ºä»€éº¼ä¸ç›´æ¥ç”¨ stats.f_oneway å°±å¥½ï¼Ÿ**\n",
        "\n",
        "- å›ç­”ï¼š å› ç‚º f_oneway ä¸»è¦æ˜¯é‡å°ã€Œå–®ä¸€å¯¦é©—ã€è¨­è¨ˆçš„ï¼Œåœ¨è™•ç†æ•¸ç™¾å€‹æ¬„ä½æ™‚ï¼Œå®ƒé »ç¹çš„åƒæ•¸æª¢æŸ¥èˆ‡è³‡æ–™è½‰æ›æœƒç”¢ç”Ÿå·¨å¤§çš„ Overheadã€‚æˆ‘å€‘æ”¹ç”¨ NumPy å‘é‡åŒ–å…¬å¼ï¼Œç›´æ¥åœ¨ C èªè¨€å±¤ç´šé‹ç®—ï¼Œæ•ˆç‡èƒ½æå‡æ•¸åå€ã€‚\n",
        "\n",
        "**Q2ï¼šå¦‚æœå…©å€‹æ¬„ä½å…§å®¹ä¸€æ¨£ä½†é †åºä¸åŒï¼Œä½ çš„ã€ŒæŒ‡ç´‹è¾¨è­˜ã€é‚„æœ‰æ•ˆå—ï¼Ÿ**\n",
        "\n",
        "- å›ç­”ï¼š ANOVA æª¢å®šæ˜¯åŸºæ–¼ Row å°æ‡‰é—œä¿‚çš„ã€‚å¦‚æœé †åºä¸åŒï¼Œä»£è¡¨å¯¦é©—è§€æ¸¬å€¼èˆ‡é¡åˆ¥çš„å°æ‡‰é—œä¿‚å·²æ”¹è®Šï¼Œè¨ˆç®—çµæœæœ¬å°±è©²ä¸åŒã€‚æ‰€ä»¥æˆ‘å€‘çš„æŒ‡ç´‹è¾¨è­˜æ¡ç”¨ tupleï¼Œç²¾æº–é–å®šã€Œå…§å®¹èˆ‡é †åºã€å®Œå…¨ä¸€è‡´çš„æ¬„ä½ï¼Œç¢ºä¿çµæœæ­£ç¢ºä¸”ä¸é‡è¤‡è¨ˆç®—ã€‚\n",
        "\n",
        "**Q3ï¼šæ•¸æ“šé‡å¤§åˆ° 20 è¬åˆ—æ™‚ï¼Œè¨˜æ†¶é«”æœƒä¸æœƒå´©æ½°ï¼Ÿ**\n",
        "\n",
        "- å›ç­”ï¼š ç³»çµ±å…·å‚™å…©å±¤ä¿è­·ã€‚ç¬¬ä¸€ï¼Œæˆ‘å€‘åœ¨è¨ˆç®—å‰æœƒå…ˆå° y é€²è¡Œ ä¸€æ¬¡æ€§æ•¸å€¼è½‰æ›ï¼Œé¿å…é‡è¤‡ä½”ç”¨ç©ºé–“ã€‚ç¬¬äºŒï¼Œåœ¨å¤šé€²ç¨‹è™•ç†æ™‚ï¼Œæˆ‘å€‘åªå‚³éå¿…è¦çš„åˆ†çµ„æ¨¡å¼ï¼ˆPatternsï¼‰ï¼Œè€Œéæ•´å€‹ DataFrameï¼Œé€™æ¥µå¤§åœ°æ¸›è¼•äº†è¨˜æ†¶é«”è² æ“”ã€‚\n",
        "\n",
        "**Q4ï¼šç‚ºä»€éº¼æ•¸æ“šé‡å°æ™‚ä¸é–‹å¹³è¡Œé‹ç®—ï¼Ÿ**\n",
        "\n",
        "- å›ç­”ï¼š é€™æ¶‰åŠã€Œé€²ç¨‹é–‹éŠ· (Process Overhead)ã€ã€‚å•Ÿå‹•é€²ç¨‹èˆ‡è³‡æ–™åºåˆ—åŒ–çš„æ™‚é–“ï¼ˆç´„ 0.3-0.5 ç§’ï¼‰ï¼Œæœ‰æ™‚æ¯”ç›´æ¥åœ¨å–®æ ¸ç®—å®Œé‚„è¦ä¹…ã€‚æˆ‘å€‘è¨­å®šäº†å‹•æ…‹é–€æª»ï¼ˆå¦‚ 500 è¬å€‹æ•¸æ“šé»ï¼‰ï¼Œç¢ºä¿ç³»çµ±å§‹çµ‚é¸æ“‡æœ€å¿«è·¯å¾‘ã€‚"
      ],
      "metadata": {
        "id": "UQwIv--WM32E"
      }
    }
  ]
}