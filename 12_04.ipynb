{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtWv8jFr84XU41gB4jksh1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annie00000/Project/blob/main/12_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ç¹ªè£½å„svidåœ– (æ‰¾ç¯„ä¾‹)"
      ],
      "metadata": {
        "id": "yIm3OM3xHcyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import os\n",
        "\n",
        "# å‡è¨­æ‚¨çš„ DataFrame å‘½åç‚º df\n",
        "# ç¢ºä¿æ‚¨çš„ df å·²ç¶“è¼‰å…¥\n",
        "\n",
        "# 1. æ‰¾å‡ºéœ€è¦ç¹ªåœ–çš„æ¬„ä½\n",
        "cols_to_exclude = ['col1', 'col2', 'col3', 'time']\n",
        "cols_to_plot = [col for col in df.columns if col not in cols_to_exclude]\n",
        "\n",
        "\n",
        "# 2. éæ­·æ¬„ä½ä¸¦ç¹ªè£½/å„²å­˜åœ–è¡¨\n",
        "output_files = []\n",
        "for col in cols_to_plot:\n",
        "    # å»ºç«‹ go.Figure() ä¸¦åŠ å…¥ go.Scatter() trace\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=df['time'],\n",
        "        y=df[col],\n",
        "        mode='markers', # ä½¿ç”¨ 'markers' é¡¯ç¤ºé»ç‹€åœ–\n",
        "        name=col\n",
        "    ))\n",
        "\n",
        "    # è¨­å®šæ¨™é¡Œå’Œè»¸æ¨™ç±¤\n",
        "    fig.update_layout(\n",
        "        title=f'{col} çš„æ™‚é–“åºåˆ—åœ–',\n",
        "        xaxis_title='time',\n",
        "        yaxis_title=col\n",
        "    )\n",
        "\n",
        "    # å„²å­˜åœ–è¡¨ç‚º HTML æª”æ¡ˆ\n",
        "    filename = f'plot_{col}.html'\n",
        "    fig.write_html(filename)\n",
        "    output_files.append(filename)\n",
        "\n",
        "print(f\"åœ–è¡¨å·²å„²å­˜ç‚º HTML æª”æ¡ˆ: {output_files}\")"
      ],
      "metadata": {
        "id": "KrPZnAx3IhN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. rule 4 : é€£çºŒ n é»ä¸Šå‡"
      ],
      "metadata": {
        "id": "B3wU8zqGJ0kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "# --- 1. æ•¸æ“šæº–å‚™ (æ¨¡æ“¬æ•¸æ“š - èˆ‡å‰æ¬¡ç›¸åŒ) ---\n",
        "data = {\n",
        "    'ctrljob': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'D', 'D', 'D', 'E', 'E', 'F', 'F'],\n",
        "    'claim_time': range(17),\n",
        "    'svid': [10.2, 11.5, 12.0, 15.5,  # A: é€£çºŒä¸Šå‡ 4 é»\n",
        "             9.0, 8.5, 1.0,           # B: æ¥µå€¼\n",
        "             10.0, 10.3, 10.5,        # C: é€£çºŒä¸Šå‡ 3 é»\n",
        "             10.0, 10.1, 10.0,        # D: ç©©å®š\n",
        "             9.9, 10.2,               # E: ç©©å®š\n",
        "             10.0, 10.1],             # F: ç©©å®š\n",
        "    'group': ['Bad'] * 10 + ['Good'] * 7\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# âœ¨ æ ¸å¿ƒå‡½å¼ (Rule 5 ç‰¹å¾µæå– - å¿…é ˆå…ˆåŸ·è¡Œä»¥æ‰¾åˆ°æœ€ä½³ n)\n",
        "# -----------------------------------------------------\n",
        "\n",
        "def get_longest_rising_sequence_info(series: pd.Series) -> Tuple[int, Optional[int]]:\n",
        "    \"\"\"è¨ˆç®—åºåˆ—ä¸­æœ€é•·é€£çºŒä¸Šå‡åºåˆ—é•·åº¦ (n) å’Œè©²åºåˆ—åœ¨åŸå§‹ DataFrame ä¸­çš„èµ·å§‹ç´¢å¼•ã€‚\"\"\"\n",
        "    if series.empty or len(series) < 2:\n",
        "        return 0, None\n",
        "\n",
        "    is_rising = series.diff().iloc[1:] > 0\n",
        "    original_indices = series.index.tolist()\n",
        "\n",
        "    max_len = 0\n",
        "    max_start_index = None\n",
        "    current_count = 0\n",
        "\n",
        "    for i, is_one in enumerate(is_rising):\n",
        "        if is_one:\n",
        "            current_count += 1\n",
        "        else:\n",
        "            if current_count + 1 > max_len and current_count >= 1:\n",
        "                max_len = current_count + 1\n",
        "                max_start_index = original_indices[i + 1 - current_count]\n",
        "\n",
        "            current_count = 0\n",
        "\n",
        "    if current_count + 1 > max_len and current_count >= 1:\n",
        "        max_len = current_count + 1\n",
        "        max_start_index = original_indices[len(series) - max_len]\n",
        "\n",
        "    return (max_len, max_start_index) if max_len >= 2 else (0, None)\n",
        "\n",
        "def get_accuracy(true_groups: pd.Series, predicted_groups: pd.Series) -> float:\n",
        "    \"\"\"è¨ˆç®—æº–ç¢ºåº¦\"\"\"\n",
        "    return accuracy_score(true_groups, predicted_groups)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 2. ç¢ºå®šæœ€ä½³æŠ˜è¡· n_critical\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# ç‰¹å¾µæå–: æ‰¾å‡ºæ¯å€‹ ctrljob æœ€é•·çš„é€£çºŒä¸Šå‡é•·åº¦ (n)\n",
        "r5_features = df.groupby('ctrljob').agg(\n",
        "    Rising_Info=('svid', get_longest_rising_sequence_info),\n",
        "    True_Group=('group', 'first')\n",
        ").reset_index()\n",
        "\n",
        "r5_features[['Max_Rising_Len', 'Start_Index']] = pd.DataFrame(\n",
        "    r5_features['Rising_Info'].tolist(),\n",
        "    index=r5_features.index\n",
        ")\n",
        "\n",
        "# å®šç¾© n_critical çš„æœç´¢ç¯„åœ\n",
        "max_len_in_data = int(r5_features['Max_Rising_Len'].max())\n",
        "n_critical_range: List[int] = list(range(2, max_len_in_data + 1))\n",
        "\n",
        "r5_results: List[Dict] = []\n",
        "for n_critical in n_critical_range:\n",
        "    r5_features['Predicted_Group'] = np.where(\n",
        "        r5_features['Max_Rising_Len'] >= n_critical,\n",
        "        'Bad',\n",
        "        'Good'\n",
        "    )\n",
        "\n",
        "    current_accuracy = get_accuracy(r5_features['True_Group'], r5_features['Predicted_Group'])\n",
        "    r5_results.append({'n_critical': n_critical, 'Accuracy': current_accuracy})\n",
        "\n",
        "r5_df = pd.DataFrame(r5_results).sort_values(by='Accuracy', ascending=False)\n",
        "\n",
        "# ğŸ† æŠ˜è¡·ç­–ç•¥é‚è¼¯ï¼šé¸æ“‡æœ€é«˜æº–ç¢ºåº¦ç¯„åœå…§çš„ä¸­é–“ n\n",
        "max_accuracy_r5 = r5_df['Accuracy'].max()\n",
        "best_r5_candidates = r5_df[r5_df['Accuracy'] == max_accuracy_r5]\n",
        "\n",
        "n_min = best_r5_candidates['n_critical'].min()\n",
        "n_max = best_r5_candidates['n_critical'].max()\n",
        "n_compromise = int(np.round((n_min + n_max) / 2))\n",
        "\n",
        "print(f\"Rule 5 æœ€ä½³æº–ç¢ºåº¦: {max_accuracy_r5:.4f}\")\n",
        "print(f\"é”åˆ°è©²æº–ç¢ºåº¦çš„ n ç¯„åœ: {n_min} åˆ° {n_max}\")\n",
        "print(f\"ä¾æŠ˜è¡·ç­–ç•¥é¸æ“‡çš„æœ€ä½³ n: **{n_compromise}**\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 3. é»å°é»åˆ¤å®šå‡½å¼ (ä½¿ç”¨ n_compromise)\n",
        "# -----------------------------------------------------\n",
        "\n",
        "def mark_rule_5_points_with_n(df: pd.DataFrame, svid_col: str, n_critical: int) -> pd.Series:\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨æœ€ä½³ n_criticalï¼Œæ¨™è¨˜ df ä¸­æ¯å€‹é»æ˜¯å¦å±¬æ–¼é•·åº¦ >= n_critical çš„é€£çºŒä¸Šå‡åºåˆ—ã€‚\n",
        "    \"\"\"\n",
        "    if n_critical < 2:\n",
        "        return pd.Series('PASS', index=df.index) # ç¢ºä¿å›å‚³å€¼ç‚º 'PASS'\n",
        "\n",
        "    r5_status = pd.Series('PASS', index=df.index)\n",
        "\n",
        "    # å¿…é ˆæ ¹æ“š ctrljob åˆ†çµ„æª¢æŸ¥\n",
        "    for job_id, job_data in df.groupby('ctrljob'):\n",
        "        series = job_data[svid_col]\n",
        "\n",
        "        if len(series) < n_critical:\n",
        "            continue\n",
        "\n",
        "        is_rising = series.diff().iloc[1:] > 0\n",
        "        original_indices = series.index.tolist()\n",
        "\n",
        "        current_count = 0 # é€£çºŒä¸Šå‡æ¬¡æ•¸ (n-1)\n",
        "\n",
        "        # éæ­·ä¸Šå‡åˆ¤æ–·åºåˆ—\n",
        "        for i, is_one in enumerate(is_rising):\n",
        "            if is_one:\n",
        "                current_count += 1\n",
        "            else:\n",
        "                # åºåˆ—ä¸­æ–·ï¼Œæª¢æŸ¥å‰ä¸€å€‹åºåˆ—\n",
        "                sequence_len = current_count + 1\n",
        "                if sequence_len >= n_critical:\n",
        "                    # æ¨™è¨˜æ•´å€‹ç¬¦åˆæ¢ä»¶çš„åºåˆ— (é•·åº¦ sequence_len)\n",
        "                    start_loc = i + 1 - sequence_len\n",
        "                    end_loc = i + 1 # exclusive end\n",
        "\n",
        "                    r5_status.loc[original_indices[start_loc : end_loc]] = 'FAIL'\n",
        "\n",
        "                current_count = 0\n",
        "\n",
        "        # æª¢æŸ¥åºåˆ—æœ«å°¾\n",
        "        sequence_len = current_count + 1\n",
        "        if sequence_len >= n_critical:\n",
        "            start_loc = len(series) - sequence_len\n",
        "            end_loc = len(series)\n",
        "\n",
        "            r5_status.loc[original_indices[start_loc : end_loc]] = 'FAIL'\n",
        "\n",
        "    return r5_status\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 4. æ‡‰ç”¨ä¸¦è¼¸å‡ºçµæœ\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# æ‡‰ç”¨æœ€ä½³ n\n",
        "df['Rule5_Status'] = mark_rule_5_points_with_n(df, 'svid', n_compromise)\n",
        "\n",
        "# è¼¸å‡ºæœ€çµ‚é»ä½åˆ¤å®šçµæœ\n",
        "final_result = df[['ctrljob', 'claim_time', 'svid', 'group', 'Rule5_Status']].copy()\n",
        "\n",
        "print(f\"## æ¯å€‹é»ä½ Rule 5 åˆ¤å®šçµæœ (ä½¿ç”¨ n={n_compromise})\")\n",
        "print(final_result.to_markdown(index=False, floatfmt=\".2f\"))"
      ],
      "metadata": {
        "id": "ikVS1HgDJ4bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### rule 4  æ”¹ç”¨ Decision Tree ä¾†æ±ºå®š n (ç”¨æœ‰ä½¿ç”¨éçš„n)"
      ],
      "metadata": {
        "id": "gE5i-wf7K7uP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**æ›¿ä»£åƒæ•¸æœç´¢ (éæ­·) çš„æ–¹æ³•**\n",
        "\n",
        "\n",
        "> åŸºæ–¼åˆ†ä½ˆåˆ†æçš„è‡¨ç•Œå€¼ (çµ±è¨ˆæ–¹æ³•):\n",
        "\n",
        "\n",
        "  å°‡å…©é‚Šé€£çºŒ $x$ é»ä¸Šå‡çš„ $x$ æ•¸ï¼Œå„è‡ªåšæˆä¸€å€‹åˆ†ä½ˆï¼Œç„¶å¾Œå– $Good$ çš„ $x$ æœ€å¤§\n",
        "å€¼èˆ‡ $Bad$ åˆ†ä½ˆçš„äº¤ç•Œã€ï¼Œé€™æŒ‡å‘äº†å°‹æ‰¾æœ€ä½³åˆ†é¡é‚Šç•Œ\n",
        "\n",
        "* æ­¥é©Ÿï¼š\n",
        "\n",
        "  1. ç‰¹å¾µæå– ($x$): é‡å°æ¯å€‹ ctrljobï¼Œè¨ˆç®—å…¶ã€Œæœ€é•·é€£çºŒä¸Šå‡é»æ•¸ ($x$)ã€ã€‚\n",
        "  2. åˆ†é›¢åˆ†ä½ˆï¼š ç²å¾— $Good$ ç¾¤çµ„çš„ $x$ åˆ†ä½ˆ ($X_{Good}$) å’Œ $Bad$ ç¾¤çµ„çš„ $x$ åˆ†ä½ˆ ($X_{Bad}$)ã€‚\n",
        "  3. å°‹æ‰¾æœ€ä½³åˆ†å‰²é» ($n$): æ‰¾åˆ°ä¸€å€‹è‡¨ç•Œå€¼ $n$ï¼Œä½¿å¾—åˆ†é¡çµæœï¼š\n",
        "    - $\\text{IF } x \\ge n \\text{ THEN Bad}$\n",
        "    - $\\text{IF } x < n \\text{ THEN Good}$\n",
        "    * èƒ½å¤ æœ€å¤§åŒ–åˆ†é¡æº–ç¢ºåº¦ã€‚é€™æœ¬è³ªä¸Šå°±æ˜¯å–®ä¸€ç‰¹å¾µçš„æœ€ä½³é–¾å€¼æœç´¢ã€‚"
      ],
      "metadata": {
        "id": "-7o0IuGKLPG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> å¦‚ä½•æ‰¾åˆ° $n$ï¼Ÿ\n",
        "\n",
        "* **å„ªåŒ–æ›¿ä»£**ï¼š æ‚¨å¯ä»¥ä½¿ç”¨ ROC æ›²ç·šæˆ– F1-Score ä½œç‚ºå„ªåŒ–æŒ‡æ¨™ï¼Œä¾†ä»£æ›¿å–®ç´”çš„ $Accuracy$ã€‚é›–ç„¶é€™ä»ç„¶æ˜¯éæ­· $n$ çš„å¯èƒ½å–å€¼ç¯„åœï¼Œä½†æœç´¢ç¯„åœè¢«é™åˆ¶åœ¨å¯¦éš›æ•¸æ“šä¸­å‡ºç¾éçš„ $x$ å€¼ï¼Œè€Œä¸æ˜¯ä»»æ„çš„ $n$ ç¯„åœï¼Œæ•ˆç‡æ›´é«˜ã€‚\n",
        "\n",
        "* **æ±ºç­–æ¨¹ (Decision Tree)**ï¼š æœ€ç°¡å–®ä¸”æœ€æœ‰æ•ˆçš„æ–¹æ³•ä¹‹ä¸€ã€‚(å–®è¦å‰‡)\n",
        "\n",
        "  1. å»ºç«‹ä¸€å€‹åƒ…ä½¿ç”¨ Max_Rising_Len ($x$) ä½œç‚ºè¼¸å…¥ç‰¹å¾µçš„æ±ºç­–æ¨¹æ¨¡å‹ï¼Œç›®æ¨™æ˜¯é æ¸¬ Group (Bad/Good)ã€‚\n",
        "  2. ç”±æ–¼åªæœ‰ä¸€å€‹ç‰¹å¾µï¼Œæ±ºç­–æ¨¹å°‡æœƒå­¸ç¿’åˆ°ä¸€å€‹æˆ–å¤šå€‹æœ€ä½³çš„å–®é‚Šè‡¨ç•Œå€¼ä¾†åˆ†å‰²æ•¸æ“šã€‚\n",
        "  3. è¼¸å‡ºï¼š æ±ºç­–æ¨¹çš„ç¬¬ä¸€å€‹åˆ†å‰²é»å°±æ˜¯æ ¹æ“š $Gini$ æˆ– $Entropy$ æ¨™æº–æ‰¾åˆ°çš„ï¼Œèƒ½å¸¶ä¾†æœ€å¤§è³‡è¨Šå¢ç›Šçš„æœ€ä½³è‡¨ç•Œå€¼ $n$ã€‚\n",
        "\n",
        "  -  Gini Impurity"
      ],
      "metadata": {
        "id": "snsZA-PpMGbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "\n",
        "# ... (Rule 5 ç‰¹å¾µæå–éƒ¨åˆ†åŒå‰ï¼Œç”Ÿæˆ r5_features) ...\n",
        "\n",
        "# æº–å‚™æ•¸æ“š for æ±ºç­–æ¨¹\n",
        "X = r5_features[['Max_Rising_Len']] # è¼¸å…¥ç‰¹å¾µ (x)\n",
        "y = r5_features['True_Group']       # ç›®æ¨™ (Good/Bad)\n",
        "\n",
        "# åˆå§‹åŒ–æ±ºç­–æ¨¹æ¨¡å‹ (æ·±åº¦è¨­ç‚º 1ï¼Œå¼·åˆ¶åªæ‰¾ä¸€å€‹åˆ†å‰²é»)\n",
        "tree_model = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "tree_model.fit(X, y)\n",
        "\n",
        "# æå–æœ€ä½³åˆ†å‰²é»\n",
        "# æ±ºç­–æ¨¹çš„ç¯€é»å±¬æ€§ä¸­åŒ…å«åˆ†å‰²é»\n",
        "best_n_split = tree_model.tree_.threshold[0]\n",
        "\n",
        "# ç”±æ–¼æ±ºç­–æ¨¹å¯èƒ½åœ¨å…©é»ä¹‹é–“åˆ†å‰²ï¼Œæˆ‘å€‘å–æ•´æ•¸ä½œç‚ºè‡¨ç•Œå€¼\n",
        "n_optimal_tree = int(np.ceil(best_n_split))\n",
        "\n",
        "# è¼¸å‡ºçµæœ\n",
        "print(f\"æ±ºç­–æ¨¹æ‰¾åˆ°çš„æœ€ä½³è‡¨ç•Œå€¼ n: {n_optimal_tree}\")"
      ],
      "metadata": {
        "id": "jd4L5eFULDtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**æœ€ä½³åƒæ•¸é¸æ“‡æ–¹æ³•**\n",
        "\n",
        "1. $n_{\\text{compromise}}$ :\n",
        "  - éæ­·æ³• (è¨ˆç®— $N_{\\text{optimal}}$ çš„ä¸­é»)\n",
        "  - æœ€å¹³è¡¡ã€‚å®ƒä»£è¡¨äº†åœ¨ä¸çŠ§ç‰²æº–ç¢ºåº¦çš„å‰æä¸‹ï¼Œæ•æ„Ÿåº¦èˆ‡ç©©å¥æ€§çš„æŠ˜è¡·é»ã€‚\n",
        "\n",
        "2. $n_{\\text{tree}}$ :\n",
        "  - æ±ºç­–æ¨¹æ³• (Gini Impurity æœ€å„ª)\n",
        "  - æœ€æ•¸å­¸/çµ±è¨ˆæœ€ä½³ã€‚å®ƒæ˜¯æ ¹æ“šè³‡è¨Šç†è«–æˆ– Gini æ¨™æº–ï¼Œå°‡ $Good/Bad$ æ•¸æ“šåˆ†å¾—æœ€ã€Œç´”æ·¨ã€çš„é‚£å€‹è‡¨ç•Œå€¼ã€‚\n",
        "\n",
        "3. $n_{\\text{min}}$ :\n",
        "  - éæ­·æ³• (è¨ˆç®— $N_{\\text{optimal}}$ çš„æœ€å°å€¼)\n",
        "  - æœ€æ•æ„Ÿçš„ SPC è¦å‰‡ã€‚å®ƒèƒ½æä¾›æœ€å¿«çš„ç•°å¸¸è­¦å ±ã€‚"
      ],
      "metadata": {
        "id": "4NBlWthRV68P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Rule 5 : é€£çºŒné»ä¸‹é™"
      ],
      "metadata": {
        "id": "kjCq2Ct-YAPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## åŸç‰ˆ\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# âœ¨ æ ¸å¿ƒå‡½å¼ (Rule 5 ç‰¹å¾µæå– - å¿…é ˆå…ˆåŸ·è¡Œä»¥æ‰¾åˆ°æœ€ä½³ n) æœ€é•·é€£çºŒä¸‹é™é•·åº¦\n",
        "# -----------------------------------------------------\n",
        "\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "def get_longest_falling_sequence_info(series: pd.Series) -> Tuple[int, Optional[int]]:\n",
        "    \"\"\"\n",
        "    Rule 6 ç‰¹å¾µæå–: è¨ˆç®—åºåˆ—ä¸­å‡ºç¾éçš„æœ€é•·çš„é€£çºŒä¸‹é™åºåˆ—é•·åº¦ (n)\n",
        "    å’Œè©²åºåˆ—åœ¨åŸå§‹ DataFrame ä¸­çš„èµ·å§‹ç´¢å¼•ã€‚\n",
        "    \"\"\"\n",
        "    if series.empty or len(series) < 2:\n",
        "        return 0, None\n",
        "\n",
        "    # é—œéµï¼šæª¢æŸ¥æ˜¯å¦ç‚ºå–®é»ä¸‹é™ (ç•¶å‰é» < å‰ä¸€é»)\n",
        "    is_falling = series.diff().iloc[1:] < 0\n",
        "    original_indices = series.index.tolist()\n",
        "\n",
        "    max_len = 0\n",
        "    max_start_index = None\n",
        "    current_count = 0    # è¨˜éŒ„é€£çºŒä¸‹é™çš„æ¬¡æ•¸ (n-1)\n",
        "\n",
        "    for i, is_one in enumerate(is_falling):\n",
        "        if is_one:\n",
        "            current_count += 1\n",
        "        else:\n",
        "            if current_count + 1 > max_len and current_count >= 1:\n",
        "                max_len = current_count + 1\n",
        "                max_start_index = original_indices[i + 1 - current_count]\n",
        "\n",
        "            current_count = 0\n",
        "\n",
        "    # è™•ç†åºåˆ—æœ«å°¾çš„é€£çºŒä¸‹é™\n",
        "    if current_count + 1 > max_len and current_count >= 1:\n",
        "        max_len = current_count + 1\n",
        "        max_start_index = original_indices[len(series) - max_len]\n",
        "\n",
        "    return (max_len, max_start_index) if max_len >= 2 else (0, None)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iJgMmYLGWbht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# 2. ç¢ºå®šæœ€ä½³æŠ˜è¡· n_critical\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# --- C. Rule 6 ç¨ç«‹å„ªåŒ– (æœç´¢æœ€ä½³ n_critical) ---\n",
        "\n",
        "# 1. ç‰¹å¾µæå–: æ‰¾å‡ºæ¯å€‹ ctrljob æœ€é•·é€£çºŒä¸‹é™é•·åº¦ (x)\n",
        "r6_features = df.groupby('ctrljob').agg(\n",
        "    Falling_Info=('svid', get_longest_falling_sequence_info),\n",
        "    True_Group=('group', 'first')\n",
        ").reset_index()\n",
        "\n",
        "r6_features[['Max_Falling_Len', 'Start_Index']] = pd.DataFrame(\n",
        "    r6_features['Falling_Info'].tolist(),\n",
        "    index=r6_features.index\n",
        ")\n",
        "\n",
        "# 2. å®šç¾© n_critical çš„æœç´¢ç¯„åœ\n",
        "max_len_in_data_r6 = int(r6_features['Max_Falling_Len'].max())\n",
        "n_critical_range_r6: List[int] = list(range(2, max_len_in_data_r6 + 1))\n",
        "\n",
        "r6_results: List[Dict] = []\n",
        "for n_critical in n_critical_range_r6:\n",
        "    # åˆ¤å®šï¼šå¦‚æœ Max_Falling_Len >= n_criticalï¼Œå‰‡åˆ¤å®šç‚º Bad\n",
        "    r6_features['Predicted_Group'] = np.where(\n",
        "        r6_features['Max_Falling_Len'] >= n_critical,\n",
        "        'Bad',\n",
        "        'Good'\n",
        "    )\n",
        "\n",
        "    current_accuracy = get_accuracy(r6_features['True_Group'], r6_features['Predicted_Group'])\n",
        "    r6_results.append({'n_critical': n_critical, 'Accuracy': current_accuracy})\n",
        "\n",
        "# 3. ğŸ† æ‡‰ç”¨æŠ˜è¡·ç­–ç•¥\n",
        "r6_df = pd.DataFrame(r6_results).sort_values(by='Accuracy', ascending=False)\n",
        "max_accuracy_r6 = r6_df['Accuracy'].max()\n",
        "best_r6_candidates = r6_df[r6_df['Accuracy'] == max_accuracy_r6]\n",
        "\n",
        "if len(best_r6_candidates) > 0:\n",
        "    n_min_r6 = best_r6_candidates['n_critical'].min()\n",
        "    n_max_r6 = best_r6_candidates['n_critical'].max()\n",
        "    n_compromise_r6 = int(np.round((n_min_r6 + n_max_r6) / 2))\n",
        "else:\n",
        "    # è™•ç†æ²’æœ‰çµæœçš„ç‰¹æ®Šæƒ…æ³\n",
        "    n_compromise_r6 = 0"
      ],
      "metadata": {
        "id": "9_2KqVvyYWVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# 3. é»å°é»åˆ¤å®šå‡½å¼ (ä½¿ç”¨ n_compromise)\n",
        "# -----------------------------------------------------\n",
        "def mark_rule_6_points_with_n(df: pd.DataFrame, svid_col: str, n_critical: int) -> pd.Series:\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨æœ€ä½³ n_criticalï¼Œæ¨™è¨˜ df ä¸­æ¯å€‹é»æ˜¯å¦å±¬æ–¼é•·åº¦ >= n_critical çš„é€£çºŒä¸‹é™åºåˆ—ã€‚\n",
        "    \"\"\"\n",
        "    if n_critical < 2:\n",
        "        return pd.Series('PASS', index=df.index)\n",
        "\n",
        "    r6_status = pd.Series('PASS', index=df.index)\n",
        "\n",
        "    for job_id, job_data in df.groupby('ctrljob'):\n",
        "        series = job_data[svid_col]\n",
        "\n",
        "        if len(series) < n_critical:\n",
        "            continue\n",
        "\n",
        "        # é—œéµï¼šæª¢æŸ¥ä¸‹é™è¶¨å‹¢\n",
        "        is_falling = series.diff().iloc[1:] < 0\n",
        "        original_indices = series.index.tolist()\n",
        "\n",
        "        current_count = 0\n",
        "\n",
        "        for i, is_one in enumerate(is_falling):\n",
        "            if is_one:\n",
        "                current_count += 1\n",
        "            else:\n",
        "                # åºåˆ—ä¸­æ–·ï¼Œæª¢æŸ¥å‰ä¸€å€‹åºåˆ—\n",
        "                sequence_len = current_count + 1\n",
        "                if sequence_len >= n_critical:\n",
        "                    # æ¨™è¨˜æ•´å€‹ç¬¦åˆæ¢ä»¶çš„åºåˆ—\n",
        "                    start_loc = i + 1 - sequence_len\n",
        "                    end_loc = i + 1\n",
        "\n",
        "                    r6_status.loc[original_indices[start_loc : end_loc]] = 'FAIL'\n",
        "\n",
        "                current_count = 0\n",
        "\n",
        "        # æª¢æŸ¥åºåˆ—æœ«å°¾\n",
        "        sequence_len = current_count + 1\n",
        "        if sequence_len >= n_critical:\n",
        "            start_loc = len(series) - sequence_len\n",
        "            end_loc = len(series)\n",
        "\n",
        "            r6_status.loc[original_indices[start_loc : end_loc]] = 'FAIL'\n",
        "\n",
        "    return r6_status"
      ],
      "metadata": {
        "id": "iFDmVkEsYs05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. æ•´åˆ Rule 4/5 çš„é€šç”¨ç¨‹å¼ç¢¼"
      ],
      "metadata": {
        "id": "tQpNhtTgYwUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# âœ¨ é€šç”¨ç‰¹å¾µæå–\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# è¨ˆç®—æœ€é•·é€£çºŒåºåˆ—çš„é•·åº¦å’Œä½ç½®ï¼Œå®ƒæ¥å—ä¸€å€‹ check_func åƒæ•¸ä¾†å®šç¾©è¶¨å‹¢æ–¹å‘\n",
        "from typing import Callable, Tuple, Optional\n",
        "# è¶¨å‹¢æª¢æŸ¥å‡½å¼çš„é¡å‹ï¼šè¼¸å…¥ Series (svid), è¼¸å‡º Series (True/False)\n",
        "TrendCheckFunc = Callable[[pd.Series], pd.Series]\n",
        "\n",
        "def get_consecutive_sequence_info(series: pd.Series, check_func: TrendCheckFunc) -> Tuple[int, Optional[int]]:\n",
        "    \"\"\"\n",
        "    é€šç”¨ç‰¹å¾µæå–: è¨ˆç®—åºåˆ—ä¸­å‡ºç¾éçš„æœ€é•·çš„é€£çºŒåºåˆ—é•·åº¦ (n) å’Œè©²åºåˆ—çš„èµ·å§‹ç´¢å¼•ã€‚\n",
        "\n",
        "    :param series: å–®ä¸€ ctrljob çš„ svid åºåˆ—ã€‚\n",
        "    :param check_func: åˆ¤æ–·è¶¨å‹¢çš„å‡½å¼ (e.g., lambda s: s.diff().iloc[1:] > 0)\n",
        "    :return: (æœ€é•·é€£çºŒé»æ•¸, èµ·å§‹ç´¢å¼•)\n",
        "    \"\"\"\n",
        "    if series.empty or len(series) < 2:\n",
        "        return 0, None\n",
        "\n",
        "    # é—œéµï¼šä½¿ç”¨å‚³å…¥çš„å‡½å¼åˆ¤æ–·è¶¨å‹¢ (ä¸Šå‡æˆ–ä¸‹é™)\n",
        "    is_trend = check_func(series)\n",
        "    original_indices = series.index.tolist()\n",
        "\n",
        "    max_len = 0\n",
        "    max_start_index = None\n",
        "    current_count = 0    # é€£çºŒè¶¨å‹¢æ¬¡æ•¸ (n-1)\n",
        "\n",
        "    # éæ­·è¶¨å‹¢åˆ¤æ–·åºåˆ—\n",
        "    for i, is_one in enumerate(is_trend):\n",
        "        if is_one:\n",
        "            current_count += 1\n",
        "        else:\n",
        "            if current_count + 1 > max_len and current_count >= 1:\n",
        "                max_len = current_count + 1\n",
        "                max_start_index = original_indices[i + 1 - current_count]\n",
        "\n",
        "            current_count = 0\n",
        "\n",
        "    # è™•ç†åºåˆ—æœ«å°¾\n",
        "    if current_count + 1 > max_len and current_count >= 1:\n",
        "        max_len = current_count + 1\n",
        "        max_start_index = original_indices[len(series) - max_len]\n",
        "\n",
        "    return (max_len, max_start_index) if max_len >= 2 else (0, None)"
      ],
      "metadata": {
        "id": "W4Y6eUoeY96h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# é€šç”¨é»ä½æ¨™è¨˜å‡½å¼ (mark_consecutive_points)\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# æœ€ä½³ $n$ æ‡‰ç”¨åˆ°é»ä½ï¼Œæ¨™è¨˜æ¯å€‹é»æ˜¯ FAIL é‚„æ˜¯ PASS\n",
        "\n",
        "def mark_consecutive_points(df: pd.DataFrame, svid_col: str, n_critical: int, check_func: TrendCheckFunc) -> pd.Series:\n",
        "    \"\"\"\n",
        "    é€šç”¨é»ä½æ¨™è¨˜: æ¨™è¨˜ df ä¸­æ¯å€‹é»æ˜¯å¦å±¬æ–¼é•·åº¦ >= n_critical çš„é€£çºŒåºåˆ—ã€‚\n",
        "    \"\"\"\n",
        "    if n_critical < 2:\n",
        "        return pd.Series('PASS', index=df.index)\n",
        "\n",
        "    r_status = pd.Series('PASS', index=df.index)\n",
        "\n",
        "    for job_id, job_data in df.groupby('ctrljob'):\n",
        "        series = job_data[svid_col]\n",
        "\n",
        "        if len(series) < n_critical:\n",
        "            continue\n",
        "\n",
        "        # é—œéµï¼šä½¿ç”¨å‚³å…¥çš„å‡½å¼åˆ¤æ–·è¶¨å‹¢\n",
        "        is_trend = check_func(series)\n",
        "        original_indices = series.index.tolist()\n",
        "\n",
        "        current_count = 0\n",
        "\n",
        "        for i, is_one in enumerate(is_trend):\n",
        "            if is_one:\n",
        "                current_count += 1\n",
        "            else:\n",
        "                sequence_len = current_count + 1\n",
        "                if sequence_len >= n_critical:\n",
        "                    # æ¨™è¨˜æ•´å€‹ç¬¦åˆæ¢ä»¶çš„åºåˆ—\n",
        "                    start_loc = i + 1 - sequence_len\n",
        "                    end_loc = i + 1\n",
        "                    r_status.loc[original_indices[start_loc : end_loc]] = 'FAIL'\n",
        "\n",
        "                current_count = 0\n",
        "\n",
        "        # æª¢æŸ¥åºåˆ—æœ«å°¾\n",
        "        sequence_len = current_count + 1\n",
        "        if sequence_len >= n_critical:\n",
        "            start_loc = len(series) - sequence_len\n",
        "            end_loc = len(series)\n",
        "            r_status.loc[original_indices[start_loc : end_loc]] = 'FAIL'\n",
        "\n",
        "    return r_status"
      ],
      "metadata": {
        "id": "epiq3Ry-ZMJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- å®šç¾©è¶¨å‹¢æª¢æŸ¥å‡½å¼ ---\n",
        "rising_check_func = lambda s: s.diff().iloc[1:] > 0\n",
        "falling_check_func = lambda s: s.diff().iloc[1:] < 0\n",
        "\n",
        "\n",
        "# --- Rule 5 (ä¸Šå‡) ç¨ç«‹å„ªåŒ– ---\n",
        "r5_features = df.groupby('ctrljob').agg(\n",
        "    Rising_Info=('svid', lambda s: get_consecutive_sequence_info(s, rising_check_func)),\n",
        "    True_Group=('group', 'first')\n",
        ").reset_index()\n",
        "# ... æ¥è‘—åŸ·è¡ŒæŠ˜è¡· n çš„æœç´¢ï¼Œå¾—åˆ° n_compromise_r5 ...\n",
        "\n",
        "# --- Rule 6 (ä¸‹é™) ç¨ç«‹å„ªåŒ– ---\n",
        "r6_features = df.groupby('ctrljob').agg(\n",
        "    Falling_Info=('svid', lambda s: get_consecutive_sequence_info(s, falling_check_func)),\n",
        "    True_Group=('group', 'first')\n",
        ").reset_index()\n",
        "# ... æ¥è‘—åŸ·è¡ŒæŠ˜è¡· n çš„æœç´¢ï¼Œå¾—åˆ° n_compromise_r6 ...\n",
        "\n",
        "\n",
        "# --- æ‡‰ç”¨åˆ°é»ä½åˆ¤å®š ---\n",
        "# df['Rule5_Status'] = mark_consecutive_points(df, 'svid', n_compromise_r5, rising_check_func)\n",
        "# df['Rule6_Status'] = mark_consecutive_points(df, 'svid', n_compromise_r6, falling_check_func)"
      ],
      "metadata": {
        "id": "WnPZ-kpQZbX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. å‘ˆä¸Šï¼Œé€šç”¨ rule 4/5 + Decision Tree"
      ],
      "metadata": {
        "id": "XlGIvqMHZwFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------\n",
        "# æ ¸å¿ƒ DT å„ªåŒ–å‡½å¼\n",
        "# -----------------------------------------------------\n",
        "# ä½¿ç”¨æ·±åº¦ç‚º 1 çš„æ±ºç­–æ¨¹ä¾†æ‰¾åˆ°æœ€ä½³åˆ†å‰²é» $n$ã€‚\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def find_optimal_n_via_dt(features_df: pd.DataFrame, feature_col: str) -> Tuple[int, float]:\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨æ±ºç­–æ¨¹ (Decision Tree, max_depth=1) è‡ªå‹•æ‰¾åˆ°å–®ä¸€ç‰¹å¾µçš„æœ€ä½³åˆ†å‰²è‡¨ç•Œå€¼ nã€‚\n",
        "\n",
        "    :param features_df: åŒ…å« Max_Consecutive_Len å’Œ True_Group çš„ DataFrameã€‚\n",
        "    :param feature_col: ç‰¹å¾µæ¬„ä½åç¨± (e.g., 'Max_Rising_Len')ã€‚\n",
        "    :return: (æœ€ä½³è‡¨ç•Œå€¼ n, è©² n é”æˆçš„ Accuracy)\n",
        "    \"\"\"\n",
        "    # æº–å‚™æ•¸æ“š\n",
        "    X = features_df[[feature_col]].fillna(0) # å¡«è£œ NaN ä»¥é˜²æ±ºç­–æ¨¹å ±éŒ¯\n",
        "    y = features_df['True_Group']\n",
        "\n",
        "    # 1. è¨“ç·´æ±ºç­–æ¨¹ (max_depth=1 å¼·åˆ¶åªæ‰¾ä¸€å€‹åˆ†å‰²é»)\n",
        "    tree_model = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "    tree_model.fit(X, y)\n",
        "\n",
        "    # 2. æå–æœ€ä½³åˆ†å‰²é» (threshold)\n",
        "    # best_n_split_raw æ˜¯æ±ºç­–æ¨¹æ‰¾åˆ°çš„æœ€ä½³åˆ†å‰²å€¼ (å¯èƒ½å¸¶å°æ•¸)\n",
        "    best_n_split_raw = tree_model.tree_.threshold[0]\n",
        "\n",
        "    # 3. ç¢ºå®šæ•´æ•¸è‡¨ç•Œå€¼ n\n",
        "    # è‹¥åˆ†å‰²é»ç‚º 3.5ï¼Œå‰‡ n >= 4 ç‚º Badï¼›è‹¥åˆ†å‰²é»ç‚º 3.0ï¼Œå‰‡ n > 3 äº¦å³ n >= 4 ç‚º Bad\n",
        "    # ä½¿ç”¨ ceil ç¢ºä¿è‡¨ç•Œå€¼ n ç¬¦åˆ 'x >= n THEN Bad' çš„é‚è¼¯\n",
        "    n_optimal = int(np.ceil(best_n_split_raw))\n",
        "\n",
        "    # 4. æ‡‰ç”¨è©² n ä¸¦è¨ˆç®— Accuracy (ä½œç‚ºæ¨¡å‹çš„æ€§èƒ½æŒ‡æ¨™)\n",
        "    features_df['Predicted_Group'] = np.where(\n",
        "        features_df[feature_col] >= n_optimal,\n",
        "        'Bad',\n",
        "        'Good'\n",
        "    )\n",
        "    accuracy = accuracy_score(y, features_df['Predicted_Group'])\n",
        "\n",
        "    return n_optimal, accuracy\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "52jHQJPkZ1GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# Rule 5 å’Œ Rule 6 çš„æ‡‰ç”¨ç¯„ä¾‹\n",
        "# -----------------------------------------------------\n",
        "\n",
        "\n",
        "# --- å®šç¾©è¶¨å‹¢æª¢æŸ¥å‡½å¼ (Rule 5/6 å…±ç”¨) ---\n",
        "rising_check_func = lambda s: s.diff().iloc[1:] > 0\n",
        "falling_check_func = lambda s: s.diff().iloc[1:] < 0\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# A. Rule 5 å„ªåŒ– (é€£çºŒä¸Šå‡)\n",
        "# -----------------------------------------------\n",
        "\n",
        "# 1. æå–ç‰¹å¾µ\n",
        "r5_features = df.groupby('ctrljob').agg(\n",
        "    Rising_Info=('svid', lambda s: get_consecutive_sequence_info(s, rising_check_func)),\n",
        "    True_Group=('group', 'first')\n",
        ").reset_index()\n",
        "r5_features[['Max_Rising_Len', 'Start_Index']] = pd.DataFrame(\n",
        "    r5_features['Rising_Info'].tolist(),\n",
        "    index=r5_features.index\n",
        ")\n",
        "\n",
        "# 2. æ‡‰ç”¨æ±ºç­–æ¨¹å°‹æ‰¾æœ€ä½³ n\n",
        "n_optimal_r5, accuracy_r5 = find_optimal_n_via_dt(r5_features.copy(), 'Max_Rising_Len')\n",
        "\n",
        "print(\"--- Rule 5 æ±ºç­–æ¨¹å„ªåŒ–çµæœ ---\")\n",
        "print(f\"æœ€ä½³è‡¨ç•Œå€¼ n (æ±ºç­–æ¨¹): {n_optimal_r5}\")\n",
        "print(f\"è©² n é”æˆçš„æº–ç¢ºåº¦: {accuracy_r5:.4f}\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# B. Rule 6 å„ªåŒ– (é€£çºŒä¸‹é™)\n",
        "# -----------------------------------------------\n",
        "\n",
        "# 1. æå–ç‰¹å¾µ\n",
        "r6_features = df.groupby('ctrljob').agg(\n",
        "    Falling_Info=('svid', lambda s: get_consecutive_sequence_info(s, falling_check_func)),\n",
        "    True_Group=('group', 'first')\n",
        ").reset_index()\n",
        "r6_features[['Max_Falling_Len', 'Start_Index']] = pd.DataFrame(\n",
        "    r6_features['Falling_Info'].tolist(),\n",
        "    index=r6_features.index\n",
        ")\n",
        "\n",
        "# 2. æ‡‰ç”¨æ±ºç­–æ¨¹å°‹æ‰¾æœ€ä½³ n\n",
        "n_optimal_r6, accuracy_r6 = find_optimal_n_via_dt(r6_features.copy(), 'Max_Falling_Len')\n",
        "\n",
        "print(\"\\n--- Rule 6 æ±ºç­–æ¨¹å„ªåŒ–çµæœ ---\")\n",
        "print(f\"æœ€ä½³è‡¨ç•Œå€¼ n (æ±ºç­–æ¨¹): {n_optimal_r6}\")\n",
        "print(f\"è©² n é”æˆçš„æº–ç¢ºåº¦: {accuracy_r6:.4f}\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# 3. æ‡‰ç”¨åˆ°é»ä½åˆ¤å®š\n",
        "# -----------------------------------------------\n",
        "# df['Rule5_Status'] = mark_consecutive_points(df, 'svid', n_optimal_r5, rising_check_func)\n",
        "# df['Rule6_Status'] = mark_consecutive_points(df, 'svid', n_optimal_r6, falling_check_func)"
      ],
      "metadata": {
        "id": "6ujNH_JUaQSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ç¸½çµèˆ‡æ³¨æ„äº‹é …**ï¼š\n",
        "\n",
        "- æ•ˆç‡æå‡: ä½¿ç”¨æ±ºç­–æ¨¹å¾Œï¼Œæ‚¨ä¸å†éœ€è¦æ‰‹å‹•éæ­· $n_{\\text{critical}}$ ç¯„åœï¼Œç¨‹å¼ç¢¼æ›´åŠ ç²¾ç°¡é«˜æ•ˆã€‚\n",
        "- å–®ä¸€è§£ï¼š æ­£å¦‚æˆ‘å€‘è¨è«–éçš„ï¼Œæ±ºç­–æ¨¹åªæœƒè¿”å›ä¸€å€‹ Gini æœ€å„ªçš„åˆ†å‰²é» $n_{\\text{optimal}}$ã€‚å¦‚æœæ‚¨éœ€è¦**ã€ŒæŠ˜è¡·ã€æˆ–ã€Œæœ€æ•æ„Ÿã€**çš„ $n$ï¼Œæ‚¨å¿…é ˆå›åˆ°æ‰‹å‹•éæ­· $Accuracy=1.0$ çš„ç¯„åœé€²è¡Œé¸æ“‡ã€‚\n",
        "- DT å…§å»ºæŒ‡æ¨™ï¼š æ±ºç­–æ¨¹é è¨­ä½¿ç”¨ Gini Impurity ä½œç‚ºå…¶å„ªåŒ–æŒ‡æ¨™ï¼Œä»¥ç¢ºä¿æ‰¾åˆ°çš„åˆ†å‰²é»èƒ½ä½¿åˆ†é¡å­é›†é”åˆ°æœ€é«˜çš„ç´”æ·¨åº¦ã€‚"
      ],
      "metadata": {
        "id": "31RKIad6aapx"
      }
    }
  ]
}