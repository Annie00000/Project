{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyzxYwCFdqSiOD1z5dfI8u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annie00000/Project/blob/main/1_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q:  \n",
        "\n",
        "è‹¥ç¾æœ‰ä¸€ä»½è³‡æ–™ï¼Œæœ‰[wafer_id, y, cluster, EPI_1/EPI_2/MES_1 ] æ¬„ä½ï¼Œå…¶ä¸­\n",
        "\n",
        "y ç‚ºå„wafer_idçš„æ¸¬é‡å€¼(valueï¼Œæ•¸å€¼)ã€‚\n",
        "\n",
        "cluster ç‚ºå„wafer_idæ‰€å±¬çš„ç¾¤é›† (good/badï¼Œé¡åˆ¥å‹)ã€‚\n",
        "\n",
        "EPI_1/EPI_2/MES_1 åˆ†åˆ¥ç‚ºwafer_idåœ¨é€™ä¸‰ç¨®é›å°ä¸‹çš„é¡åˆ¥è³‡æ–™\n",
        "\n",
        "æƒ³è¦åˆ†æ å“ªå€‹é›æ’æ¬„ä½èˆ‡wafer_id, y, cluster çš„ ç›¸é—œæ€§æœ€å¤§ ï¼Œäº¦æˆ–æ˜¯èªª é€ æˆé€™clusterç‹€æ…‹ æ˜¯å“ªå€‹æ©Ÿå°çš„å½±éŸ¿æœ€"
      ],
      "metadata": {
        "id": "Op_bqh7Hitpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "TzL_eKAFi2Z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ä¸€ã€ç¶œåˆæŒ‡æ¨™"
      ],
      "metadata": {
        "id": "MuRVOvt8jQlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. ç¶œåˆæŒ‡æ¨™:**\n",
        "\n",
        "$$Score = w_1 \\cdot \\eta^2 (\\text{Mean Shift}) + w_2 \\cdot \\text{Cramer's V} (\\text{Category Match}) + w_3 \\cdot \\eta^2_{var} (\\text{Variance Shift})$$\n",
        "\n",
        "(æ³¨æ„: ä¿‚æ•¸ç¸½å’Œé ˆç‚º1ï¼Œæˆ–è€…ä¸‹æ–¹é™¤ä¿‚æ•¸ç¸½å’Œ)\n",
        "\n",
        "\n",
        "\n",
        "**2. æŒ‡æ¨™æ„ç¾©**\n",
        "  - $\\eta^2$ (å‡å€¼è§£é‡‹åŠ›): è©²å› å­å„è™•ç† vs. $y$\n",
        "    - å®šç¾©ï¼š æ©Ÿå° vs. $y$ã€‚è¡¡é‡ã€Œæ¬„ä½ã€å°ã€Œ$y$ å€¼ä¸­å¿ƒè¶¨å‹¢ã€çš„è§£é‡‹ç¨‹åº¦ã€‚\n",
        "    - æ„ç¾©ï¼š è‹¥æ•¸å€¼è¼ƒé«˜ï¼Œä»£è¡¨è©²æ©Ÿå°æ˜¯é€ æˆ $y$ å€¼å‡ºç¾**ã€Œä½ç§» (Mean Shift)ã€**çš„ä¸»è¦åŸå› ï¼ˆä¾‹å¦‚ï¼šæ©Ÿå° A ç”¢å‡ºå€¼åå¤§ï¼Œæ©Ÿå° B åå°ï¼‰ã€‚\n",
        "  - $\\eta^2_{var}$  (ç©©å®šåº¦è§£é‡‹åŠ›): è©²å› å­å„è™•ç† vs. $y$\n",
        "    - å®šç¾©ï¼š è¡¡é‡ã€Œæ¬„ä½ã€å°ã€Œ$y$ å€¼æ³¢å‹•åŠ‡çƒˆç¨‹åº¦ã€çš„è§£é‡‹ç¨‹åº¦ã€‚\n",
        "    - æ„ç¾©ï¼š è‹¥æ•¸å€¼è¼ƒé«˜ï¼Œä»£è¡¨è©²æ©Ÿå°æ±ºå®šäº†ç”Ÿç”¢çš„**ã€Œç©©å®šæ€§ã€**ã€‚é€™èƒ½ç²¾æº–æ•æ‰åˆ°ä½ æåˆ°çš„ã€Œä¸€ç¾¤ç©©å®šã€ä¸€ç¾¤æ³¢å‹•ã€ä¹‹ç¾è±¡ï¼Œè­˜åˆ¥å‡ºå“ªå€‹æ©Ÿå°å°è‡´äº†è£½ç¨‹å‹æ…‹çš„åˆ†æ•£ã€‚\n",
        "  - Cramer's V (é¡åˆ¥ä¸€è‡´æ€§): æ©Ÿå° vs. Clusterã€‚\n",
        "    - å®šç¾©ï¼š è©²æ©Ÿå°çš„é¡åˆ¥èˆ‡ Good/Bad çš„åˆ†é¡æ¨™ç±¤æœ‰å¤šå»åˆã€‚\n",
        "    - æ„ç¾©ï¼š è¡¡é‡æ©Ÿå°ç·¨è™Ÿèˆ‡ Good/Bad æ¨™ç±¤çš„ä¸€è‡´æ€§ã€‚è‹¥æ•¸å€¼æ¥è¿‘ 1ï¼Œä»£è¡¨è©²æ©Ÿå°çš„åˆ†çµ„çµæœå¹¾ä¹èƒ½å®Œç¾é æ¸¬ï¼ˆæˆ–å°æ‡‰ï¼‰è©² Wafer æœƒè¢«æ­¸é¡ç‚ºå“ªä¸€å€‹ Clusterã€‚\n",
        "\n",
        "\n",
        "**3. ANOVAï¼Œç‚ºä»€éº¼é¸æ“‡ $\\eta^2$ (Eta-square) è€Œé p-valueï¼Ÿ**\n",
        "\n",
        "- æ•ˆæ‡‰å€¼ (Effect Size) vs. é¡¯è‘—æ€§ï¼š\n",
        "\n",
        "  - p-value åªèƒ½å‘Šè¨´ä½ ã€Œæ©Ÿå°å° $y$ æ˜¯å¦æœ‰å½±éŸ¿ã€ï¼Œä½†å®ƒæ¥µåº¦ä¾è³´æ¨£æœ¬æ•¸ã€‚å¦‚æœè³‡æ–™é‡å¾ˆå¤§ï¼Œå³ä¾¿å½±éŸ¿å¾®ä¹å…¶å¾®ï¼Œp-value ä¹Ÿæœƒè¶¨è¿‘æ–¼ 0ã€‚å®ƒç„¡æ³•å‘Šè¨´ä½ å½±éŸ¿çš„ã€Œç¨‹åº¦ã€ã€‚\n",
        "  - $\\eta^2$ è¡¡é‡çš„æ˜¯ **ã€Œè®Šç•°è§£é‡‹æ¯”ä¾‹ã€**ã€‚ä¾‹å¦‚ $\\eta^2 = 0.8$ ä»£è¡¨ $y$ çš„ç¸½è®Šç•°ä¸­æœ‰ 80% æ˜¯ç”±è©²æ©Ÿå°é€ æˆçš„ã€‚é€™ç›´æ¥å°æ‡‰åˆ°ä½ æƒ³è¦çš„ã€Œå½±éŸ¿æœ€æ·±ã€çš„æ¦‚å¿µã€‚\n",
        "\n",
        "- å°ºåº¦çµ±ä¸€ï¼š\n",
        "  - $\\eta^2$ çš„ç¯„åœå›ºå®šåœ¨ $0 \\sim 1$ ä¹‹é–“ã€‚\n",
        "  - Cramer's V çš„ç¯„åœä¹Ÿæ˜¯ $0 \\sim 1$ã€‚\n",
        "  - å…©è€…å°ºåº¦ç›¸åŒï¼Œéå¸¸æ–¹ä¾¿ç›´æ¥é€²è¡ŒåŠ æ¬Šè¨ˆç®—æˆ–ç¶œåˆè©•åˆ†ï¼›è€Œ p-value çš„å°ºåº¦ï¼ˆé€šå¸¸æ˜¯æ¥µå°å€¼ï¼‰å¾ˆé›£ç›´æ¥èˆ‡å…¶ä»–æŒ‡æ¨™èåˆã€‚"
      ],
      "metadata": {
        "id": "QlI2LsQUj3At"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. è£œå……-Levene's Test:**\n",
        "\n",
        "  - æª¢å®šæ‰€è¬‚çš„ã€Œè®Šç•°æ•¸åŒè³ªæ€§ã€ï¼Œæª¢é©—å„ç¾¤ä¹‹é–“ç©©å®šåº¦æ˜¯å¦æœ‰é¡¯è‘—å·®ç•°ã€‚\n",
        "  - ç‚ºä½•å°'éæ­£æ…‹'è³‡æ–™ç‰¹åˆ¥æœ‰æ•ˆï¼Œé€™æ˜¯å› ç‚ºå…¶ã€Œè®Šç•°æ•¸çš„æª¢å®šã€è½‰æ›æˆã€Œå°é›¢å·®ï¼ˆDeviationsï¼‰é€²è¡Œ ANOVAã€\n",
        "\n"
      ],
      "metadata": {
        "id": "K5HCZHHMBtKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "tdZxpeq5J1Oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* å‚™è¨»: VBGMM åˆ†ç¾¤çš„ï¼Œé€™æ„å‘³è‘— Cluster æœ¬èº«å°±åŒ…å«äº†ã€Œå‡å€¼ï¼ˆMeanï¼‰ã€èˆ‡ã€Œè®Šç•°æ•¸ï¼ˆVarianceï¼‰ã€çš„ç‰¹å¾µã€‚ æ‰€ä»¥ä½¿ç”¨é€™å¥—**ã€Œä¸‰åˆä¸€ç¶œåˆè©•åˆ†æŒ‡æ¨™ã€**ï¼ˆEta-square + Cramer's V + Variance Effectï¼‰éå¸¸é©åˆ"
      ],
      "metadata": {
        "id": "1kNb3KyMJ2Qo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "RZkWGiZYJ-tl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ç¨‹å¼ç¢¼**"
      ],
      "metadata": {
        "id": "X9Oc5X53Op1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. åŸºç¤"
      ],
      "metadata": {
        "id": "S8BzEUoIOsJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def analyze_machine_impact(df, setting_dict):\n",
        "    \"\"\"\n",
        "    åˆ†ææ©Ÿå°å› å­å°æ•¸å€¼åç§»(Mean)ã€æ³¢å‹•(Variance)åŠåˆ†ç¾¤(Cluster)çš„ç¶œåˆå½±éŸ¿åŠ›\n",
        "    \"\"\"\n",
        "    # æå–åƒæ•¸\n",
        "    value_col = setting_dict['value_colname']\n",
        "    cluster_col = setting_dict['cluster_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "    w1, w2, w3 = setting_dict['proportion']\n",
        "\n",
        "    # è¨ˆç®—çµ•å°é›¢å·® (ç”¨æ–¼è¡¡é‡æ³¢å‹• Variance çš„è§£é‡‹åŠ›)\n",
        "    # ä½¿ç”¨ median æ˜¯ç‚ºäº†å°é›¢ç¾¤å€¼æ›´ç©©å¥ (Robust)\n",
        "    y_dev = np.abs(df[value_col] - df[value_col].median())\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for factor in factors:\n",
        "        # --- 1. Eta-squared (Mean) ---\n",
        "        grand_mean = df[value_col].mean()\n",
        "        sst_mean = np.sum((df[value_col] - grand_mean)**2)\n",
        "        group_stats = df.groupby(factor)[value_col].agg(['mean', 'count'])\n",
        "        ssb_mean = np.sum(group_stats['count'] * (group_stats['mean'] - grand_mean)**2)\n",
        "        eta_mean = ssb_mean / sst_mean if sst_mean != 0 else 0\n",
        "\n",
        "        # --- 2. Eta-squared (Variance / Volatility) ---\n",
        "        grand_mean_dev = y_dev.mean()\n",
        "        sst_var = np.sum((y_dev - grand_mean_dev)**2)\n",
        "        group_stats_dev = pd.DataFrame({'dev': y_dev, 'grp': df[factor]}).groupby('grp')['dev'].agg(['mean', 'count'])\n",
        "        ssb_var = np.sum(group_stats_dev['count'] * (group_stats_dev['mean'] - grand_mean_dev)**2)\n",
        "        eta_var = ssb_var / sst_var if sst_var != 0 else 0\n",
        "\n",
        "        # --- 3. Cramer's V (Cluster Association) ---\n",
        "        confusion_matrix = pd.crosstab(df[factor], df[cluster_col])\n",
        "        chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "        n = len(df)\n",
        "        phi2 = chi2 / n\n",
        "        r, k = confusion_matrix.shape\n",
        "        # Bias-correction\n",
        "        phi2_corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "        r_corr = r - ((r-1)**2)/(n-1)\n",
        "        k_corr = k - ((k-1)**2)/(n-1)\n",
        "\n",
        "        denom = min((k_corr-1), (r_corr-1))\n",
        "        cv = np.sqrt(phi2_corr / denom) if denom > 0 else 0\n",
        "\n",
        "        # --- ç¶œåˆå¾—åˆ† ---\n",
        "        score = (eta_mean * w1) + (eta_var * w2) + (cv * w3)\n",
        "\n",
        "        results.append({\n",
        "            'Factor': factor,\n",
        "            'Eta_Mean': round(eta_mean, 4),\n",
        "            'Eta_Var': round(eta_var, 4),\n",
        "            'Cramers_V': round(cv, 4),\n",
        "            'Score': round(score, 4)\n",
        "        })\n",
        "\n",
        "    # å»ºç«‹å›å‚³ DataFrame ä¸¦æ’åºèˆ‡æ’å\n",
        "    result_df = pd.DataFrame(results)\n",
        "    result_df = result_df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "    result_df['Rank'] = result_df['Score'].rank(ascending=False, method='min').astype(int)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# --- ä½¿ç”¨ç¯„ä¾‹ ---\n",
        "# setting_dict = {\n",
        "#     'value_colname': 'y',\n",
        "#     'cluster_colname': 'cluster',\n",
        "#     'factor_col_list': ['mes_1', 'mes_2', 'pms_1', 'pms_2'],\n",
        "#     'proportion': [0.333, 0.333, 0.334]\n",
        "# }\n",
        "# report = analyze_machine_impact(my_df, setting_dict)\n",
        "# print(report)"
      ],
      "metadata": {
        "id": "hUQWQHfMJ1uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "é€™ä»½å›å‚³æ¸…å–®èƒ½å¹«ä½ å€åˆ†å‡ºã€Œæ•…éšœçš„é¡å‹ã€ï¼š\n",
        "- å¦‚æœæŸå› å­ Eta_Mean è¶…é«˜ï¼š ä»£è¡¨è©²æ©Ÿå°æ˜¯é€ æˆ $y$ å€¼æ•´é«”åç§»ï¼ˆä¾‹å¦‚åšåº¦è®Šè–„ã€é˜»å€¼è®Šå¤§ï¼‰çš„ä¸»å› ã€‚\n",
        "- å¦‚æœæŸå› å­ Eta_Var è¶…é«˜ï¼š ä»£è¡¨è©²æ©Ÿå°æ˜¯é€ æˆã€Œè£½ç¨‹ä¸ç©©å®šã€çš„ä¸»å› ã€‚å³ä¾¿å¹³å‡å€¼æ²’è®Šï¼Œå®ƒè®“è³‡æ–™è®Šå¾—éå¸¸æ•£ã€‚\n",
        "- å¦‚æœ Cramers_V è·Ÿå‰å…©è€…éƒ½é«˜ï¼š ä»£è¡¨é€™å°æ©Ÿå°å°±æ˜¯ VBGMM èƒ½åˆ†å‡ºå…©ç¾¤çš„é—œéµåˆ†æ°´å¶ºã€‚"
      ],
      "metadata": {
        "id": "mwjJZwQzPPLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _calculate_eta_sq(df, cat_col, num_col, sst):\n",
        "    \"\"\"\n",
        "    åŠ å¼·ç‰ˆ Eta-squared è¨ˆç®—\n",
        "    è™•ç†ï¼šç¼ºå¤±å€¼ã€å–®ä¸€åˆ†é¡ã€SSTç‚º0ç­‰ç•°å¸¸\n",
        "    \"\"\"\n",
        "    # 1. è™•ç†æ©Ÿå°å› å­ç¼ºå¤±å€¼ï¼šå°‡å…¶è¦–ç‚ºä¸€å€‹ç¨ç«‹é¡åˆ¥\n",
        "    # ä½¿ç”¨ copy() é¿å… SettingWithCopyWarning\n",
        "    temp_series = df[cat_col].fillna('NA')\n",
        "\n",
        "    # 2. æª¢æŸ¥åˆ†é¡æ•¸é‡ï¼šè‹¥åªæœ‰ä¸€å€‹é¡åˆ¥ï¼Œç„¡æ³•è¨ˆç®—è®Šç•°è§£é‡‹åŠ›\n",
        "    unique_cats = temp_series.unique()\n",
        "    if len(unique_cats) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    # 3. æª¢æŸ¥ç¸½è®Šç•° (SST)ï¼šè‹¥ SST ç‚º 0ï¼Œä»£è¡¨ y å€¼å…¨éƒ¨ç›¸åŒ\n",
        "    if sst <= 0:\n",
        "        return 0.0\n",
        "\n",
        "    # 4. è¨ˆç®— SSB (çµ„é–“å¹³æ–¹å’Œ)\n",
        "    grand_mean = df[num_col].mean()\n",
        "    # é€™è£¡ä½¿ç”¨ temp_series ä½œç‚ºåˆ†çµ„ä¾æ“š\n",
        "    group_stats = df[num_col].groupby(temp_series).agg(['mean', 'count'])\n",
        "\n",
        "    # SSB = sum( n_i * (mean_i - grand_mean)^2 )\n",
        "    ssb = np.sum(group_stats['count'] * (group_stats['mean'] - grand_mean)**2)\n",
        "\n",
        "    # 5. è¨ˆç®— Eta_sq ä¸¦ç¢ºä¿ä¸è¶…é 1.0 (é¿å…æµ®é»é‹ç®—èª¤å·®)\n",
        "    eta_sq = min(ssb / sst, 1.0)\n",
        "\n",
        "    return eta_sq"
      ],
      "metadata": {
        "id": "nXD5Kmf6WlZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _calculate_cramers_v(df, col1, col2):\n",
        "    \"\"\"å…§éƒ¨å‡½æ•¸ï¼šè¨ˆç®—åå·®ä¿®æ­£å¾Œçš„ Cramer's V\"\"\"\n",
        "    confusion_matrix = pd.crosstab(df[col1], df[col2])\n",
        "    if confusion_matrix.empty: return 0\n",
        "\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = len(df)\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "\n",
        "    # Bias-correction\n",
        "    phi2_corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    r_corr = r - ((r-1)**2)/(n-1)\n",
        "    k_corr = k - ((k-1)**2)/(n-1)\n",
        "\n",
        "    denom = min((k_corr-1), (r_corr-1))\n",
        "    return np.sqrt(phi2_corr / denom) if denom > 0 else 0"
      ],
      "metadata": {
        "id": "0AEMlmhQWwB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_machine_impact(df, setting_dict):\n",
        "    # 1. åƒæ•¸æå–\n",
        "    v_col = setting_dict['value_colname']\n",
        "    c_col = setting_dict['cluster_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "    w1, w2, w3 = setting_dict['proportion']\n",
        "\n",
        "    # 2. ç¼ºå¤±å€¼èˆ‡è³‡æ–™æª¢æŸ¥\n",
        "    relevant_cols = [v_col, c_col] + factors\n",
        "    missing_info = df[relevant_cols].isnull().sum()\n",
        "    if missing_info.any():\n",
        "        print(\"âš ï¸ è­¦å‘Šï¼šåµæ¸¬åˆ°ç¼ºå¤±å€¼ï¼Œåˆ†æå°‡æš«æ™‚æ’é™¤é€™äº›è§€æ¸¬å€¼ã€‚\")\n",
        "        print(missing_info[missing_info > 0])\n",
        "        df = df.dropna(subset=relevant_cols).copy()\n",
        "\n",
        "    # 3. é è™•ç†ï¼šè¨ˆç®—çµ•å°é›¢å·®èˆ‡å…¨åŸŸ SST\n",
        "    df['y_dev'] = np.abs(df[v_col] - df[v_col].median())\n",
        "\n",
        "    # é å…ˆè¨ˆç®— SSTï¼Œé¿å…è¿´åœˆé‡è¤‡é‹ç®—\n",
        "    sst_mean = np.sum((df[v_col] - df[v_col].mean())**2)\n",
        "    sst_var = np.sum((df['y_dev'] - df['y_dev'].mean())**2)\n",
        "\n",
        "    # 4. è¿´åœˆè¨ˆç®—å„å› å­å¾—åˆ†\n",
        "    results = []\n",
        "    for factor in factors:\n",
        "        e_mean = _calculate_eta_sq(df, factor, v_col, sst_mean)\n",
        "        e_var = _calculate_eta_sq(df, factor, 'y_dev', sst_var)\n",
        "        c_v = _calculate_cramers_v(df, factor, c_col)\n",
        "\n",
        "        score = (e_mean * w1) + (e_var * w2) + (c_v * w3)\n",
        "\n",
        "        results.append({\n",
        "            'Factor': factor,\n",
        "            'Eta_Mean': e_mean,\n",
        "            'Eta_Var': e_var,\n",
        "            'Cramers_V': c_v,\n",
        "            'Score': score\n",
        "        })\n",
        "\n",
        "    # 5. æ’åèˆ‡æ ¼å¼åŒ–\n",
        "    res_df = pd.DataFrame(results).sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "    res_df['Rank'] = res_df['Score'].rank(ascending=False, method='min').astype(int)\n",
        "\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "T0SWyoPBWzeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. æ–°å¢ hashæ©Ÿåˆ¶"
      ],
      "metadata": {
        "id": "bbaQfbWncSN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def _get_column_hash(series):\n",
        "    \"\"\"å°‡æ¬„ä½å…§å®¹è½‰åŒ–ç‚ºå”¯ä¸€çš„æŒ‡ç´‹ (MD5 Hash)\"\"\"\n",
        "    # è™•ç†ç¼ºå¤±å€¼å¾Œè½‰ç‚º tuple ä¸¦é€²è¡Œé›œæ¹Š\n",
        "    # ä½¿ç”¨ tuple æ˜¯å› ç‚ºå®ƒæ˜¯ immutableï¼Œé›œæ¹Šé€Ÿåº¦å¿«\n",
        "    # å¾Œé€²è¡Œ MD5 é›œæ¹Šï¼Œé¿å…ç›´æ¥æ“ä½œå·¨å¤§çš„ tuple\n",
        "    data_tuple = tuple(series.fillna('NA').values) # series.fillna('NA').astype(str).values.tobytes()\n",
        "    return hashlib.md5(str(data_tuple).encode()).hexdigest()\n",
        "\n",
        "def analyze_machine_impact_optimized(df, setting_dict):\n",
        "    v_col = setting_dict['value_colname']\n",
        "    c_col = setting_dict['cluster_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "    w1, w2, w3 = setting_dict['proportion']\n",
        "\n",
        "    # 1. é è™•ç†èˆ‡å…¨åŸŸ SST è¨ˆç®—\n",
        "    df = df.copy()\n",
        "    df['y_dev'] = np.abs(df[v_col] - df[v_col].median())\n",
        "    sst_mean = np.sum((df[v_col] - df[v_col].mean())**2)\n",
        "    sst_var = np.sum((df['y_dev'] - df['y_dev'].mean())**2)\n",
        "\n",
        "    # 2. å¿«å–èˆ‡æŒ‡ç´‹æ˜ å°„å­—å…¸\n",
        "    cache = {}         # å„²å­˜ {hash: {'eta_mean': x, 'eta_var': y, 'cv': z}}\n",
        "    column_hashes = {} # å„²å­˜ {column_name: hash}\n",
        "    results = []\n",
        "\n",
        "    print(f\"ğŸš€ é–‹å§‹åˆ†æ {len(factors)} å€‹æ¬„ä½...\")\n",
        "\n",
        "    # 3. åŸ·è¡Œè¿´åœˆ\n",
        "    for factor in factors:\n",
        "        # A. å–å¾—è©²æ¬„ä½çš„å…§å®¹æŒ‡ç´‹\n",
        "        col_hash = _get_column_hash(df[factor])\n",
        "        column_hashes[factor] = col_hash\n",
        "\n",
        "        # B. æª¢æŸ¥å¿«å– (Caching Mechanism)\n",
        "        if col_hash in cache:\n",
        "            stats = cache[col_hash]\n",
        "            is_cached = True\n",
        "        else:\n",
        "            # C. è¨ˆç®—é‚è¼¯ (åªæœ‰å¿«å–æœªå‘½ä¸­æ™‚æ‰åŸ·è¡Œ)\n",
        "            e_mean = _calculate_eta_sq(df, factor, v_col, sst_mean)\n",
        "            e_var = _calculate_eta_sq(df, factor, 'y_dev', sst_var)\n",
        "            c_v = _calculate_cramers_v(df, factor, c_col)\n",
        "\n",
        "            stats = {\n",
        "                'Eta_Mean': e_mean,\n",
        "                'Eta_Var': e_var,\n",
        "                'Cramers_V': c_v,\n",
        "                'Unique_Cats': df[factor].fillna('NA').nunique()\n",
        "            }\n",
        "            cache[col_hash] = stats\n",
        "            is_cached = False\n",
        "\n",
        "        # D. å½™æ•´çµæœ\n",
        "        score = (stats['Eta_Mean'] * w1) + (stats['Eta_Var'] * w2) + (stats['Cramers_V'] * w3)\n",
        "\n",
        "        results.append({\n",
        "            'Factor': factor,\n",
        "            'Hash': col_hash,\n",
        "            'Unique_Cats': stats['Unique_Cats'],\n",
        "            'Eta_Mean': stats['Eta_Mean'],\n",
        "            'Eta_Var': stats['Eta_Var'],\n",
        "            'Cramers_V': stats['Cramers_V'],\n",
        "            'Score': score,\n",
        "            'Cached': is_cached  # æ¨™è¨»æ˜¯å¦ä½¿ç”¨äº†å¿«å–\n",
        "        })\n",
        "\n",
        "    # 4. æ’åºèˆ‡è¼¸å‡º\n",
        "    res_df = pd.DataFrame(results).sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "    res_df['Rank'] = res_df['Score'].rank(ascending=False, method='min').astype(int)\n",
        "\n",
        "    # çµ±è¨ˆå„ªåŒ–æ•ˆç‡\n",
        "    cache_hits = sum(res_df['Cached'])\n",
        "    print(f\"âœ… åˆ†æå®Œæˆï¼æ¬„ä½ç¸½æ•¸: {len(factors)}, å¿«å–å‘½ä¸­: {cache_hits} (çœä¸‹ {round(cache_hits/len(factors)*100, 1)}% è¨ˆç®—é‡)\")\n",
        "\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "FjdIqvMxcU2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. multiprocessing"
      ],
      "metadata": {
        "id": "vo4mz2rfd5aC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "åœ¨ä½¿ç”¨ concurrent.futures.ProcessPoolExecutor æ­é…æˆ‘å€‘ä¹‹å‰çš„ çµæœå¿«å– (Caching) èˆ‡ æŒ‡ç´‹è¾¨è­˜ (Hashing) é‚è¼¯æ™‚ï¼Œæœ‰ä¸€å€‹æ ¸å¿ƒé—œéµï¼š**ã€Œå…ˆå»é‡ï¼Œå†ä¸¦è¡Œã€**ã€‚\n",
        "\n",
        "å¦‚æœç›´æ¥æŠŠ 2000 å€‹æ¬„ä½ä¸Ÿé€²å¤šæ ¸é‹ç®—ï¼Œç›¸åŒçš„æ¬„ä½æœƒè¢«é‡è¤‡è¨ˆç®—å¤šæ¬¡ï¼Œé€ æˆ CPU è³‡æºæµªè²»ã€‚æœ€è°æ˜çš„åšæ³•æ˜¯ï¼š\n",
        "\n",
        "1. æŒ‡ç´‹è­˜åˆ¥ï¼šåœ¨ä¸»é€²ç¨‹å…ˆè¨ˆç®—æ‰€æœ‰æ¬„ä½çš„ Hashï¼Œæ‰¾å‡ºã€Œä¸é‡è¤‡çš„å…§å®¹ã€ã€‚\n",
        "\n",
        "2. æ´¾ç™¼ä»»å‹™ï¼šåªå°‡ã€Œä¸é‡è¤‡çš„å…§å®¹ã€æ´¾ç™¼çµ¦å„å€‹æ ¸å¿ƒè¨ˆç®—çµ±è¨ˆæŒ‡æ¨™ã€‚\n",
        "\n",
        "3. çµæœæ˜ å°„ï¼šé‹ç®—å®Œæˆå¾Œï¼Œå†å°‡çµæœæ˜ å°„å›åŸæœ¬çš„ 2000 å€‹æ¬„ä½ã€‚"
      ],
      "metadata": {
        "id": "FG1-_PzpejQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1. ç‰ˆæœ¬1"
      ],
      "metadata": {
        "id": "3EDjgtLzfPTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from scipy.stats import chi2_contingency\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import os\n",
        "\n",
        "# --- æ ¸å¿ƒè¨ˆç®—å­ç¨‹å¼ (éœ€æ”¾åœ¨ Top-level ä»¥ä¾¿ Multiprocessing åºåˆ—åŒ–) ---\n",
        "\n",
        "def _core_compute_engine(data_dict):\n",
        "    \"\"\"\n",
        "    åœ¨å­é€²ç¨‹ä¸­é‹è¡Œçš„è¨ˆç®—å¼•æ“\n",
        "    \"\"\"\n",
        "    factor_name = data_dict['factor_name']\n",
        "    factor_series = data_dict['factor_series']\n",
        "    y_series = data_dict['y_series']\n",
        "    y_dev_series = data_dict['y_dev_series']\n",
        "    cluster_series = data_dict['cluster_series']\n",
        "    sst_mean = data_dict['sst_mean']\n",
        "    sst_var = data_dict['sst_var']\n",
        "\n",
        "    # 1. Eta_Mean\n",
        "    e_mean = _calculate_eta_sq_logic(factor_series, y_series, sst_mean)\n",
        "    # 2. Eta_Var\n",
        "    e_var = _calculate_eta_sq_logic(factor_series, y_dev_series, sst_var)\n",
        "    # 3. Cramer's V\n",
        "    cv = _calculate_cramers_v_logic(factor_series, cluster_series)\n",
        "\n",
        "    return {\n",
        "        'fingerprint': data_dict['fingerprint'],\n",
        "        'stats': {\n",
        "            'e_m': e_mean,\n",
        "            'e_v': e_var,\n",
        "            'cv': cv,\n",
        "            'u': factor_series.nunique()\n",
        "        }\n",
        "    }\n",
        "\n",
        "def _calculate_eta_sq_logic(cat_series, num_series, sst):\n",
        "    if sst <= 0: return 0.0\n",
        "    cat_filled = cat_series.fillna('NA')\n",
        "    if cat_filled.nunique() < 2: return 0.0\n",
        "\n",
        "    grand_mean = num_series.mean()\n",
        "    group_means = num_series.groupby(cat_filled).agg(['mean', 'count'])\n",
        "    ssb = np.sum(group_means['count'] * (group_means['mean'] - grand_mean)**2)\n",
        "    return min(ssb / sst, 1.0)\n",
        "\n",
        "def _calculate_cramers_v_logic(cat_series, cluster_series):\n",
        "    confusion_matrix = pd.crosstab(cat_series.fillna('NA'), cluster_series)\n",
        "    if confusion_matrix.empty or min(confusion_matrix.shape) < 2: return 0.0\n",
        "\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = len(cat_series)\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    phi2_corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    r_corr = r - ((r-1)**2)/(n-1)\n",
        "    k_corr = k - ((k-1)**2)/(n-1)\n",
        "    denom = min((k_corr-1), (r_corr-1))\n",
        "    return np.sqrt(phi2_corr / denom) if denom > 0 else 0\n",
        "\n",
        "# --- ä¸»ç¨‹å¼ ---\n",
        "\n",
        "def analyze_machine_impact_parallel(df, setting_dict):\n",
        "    v_col = setting_dict['value_colname']\n",
        "    c_col = setting_dict['cluster_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "    w1, w2, w3 = setting_dict['proportion']\n",
        "    max_workers = setting_dict.get('max_workers', os.cpu_count())\n",
        "\n",
        "    # 1. é è¨ˆç®—å…¨åŸŸè®Šæ•¸\n",
        "    y_dev = np.abs(df[v_col] - df[v_col].median())\n",
        "    sst_mean = np.sum((df[v_col] - df[v_col].mean())**2)\n",
        "    sst_var = np.sum((y_dev - y_dev.mean())**2)\n",
        "\n",
        "    # 2. æŒ‡ç´‹è¾¨è­˜ï¼šæ‰¾å‡ºçœŸæ­£éœ€è¦è¨ˆç®—çš„å”¯ä¸€æ¬„ä½\n",
        "    unique_tasks = {}\n",
        "    factor_to_hash = {}\n",
        "\n",
        "    for f in factors:\n",
        "        # ä½¿ç”¨ tobytes() å¿«é€Ÿé›œæ¹Š\n",
        "        f_hash = hashlib.md5(df[f].fillna('NA').astype(str).values.tobytes()).hexdigest()\n",
        "        factor_to_hash[f] = f_hash\n",
        "        if f_hash not in unique_tasks:\n",
        "            unique_tasks[f_hash] = f\n",
        "\n",
        "    # 3. æº–å‚™ä¸¦è¡Œä»»å‹™æ¸…å–®\n",
        "    task_list = []\n",
        "    for f_hash, f_name in unique_tasks.items():\n",
        "        task_list.append({\n",
        "            'fingerprint': f_hash,\n",
        "            'factor_name': f_name,\n",
        "            'factor_series': df[f_name],\n",
        "            'y_series': df[v_col],\n",
        "            'y_dev_series': y_dev,\n",
        "            'cluster_series': df[c_col],\n",
        "            'sst_mean': sst_mean,\n",
        "            'sst_var': sst_var\n",
        "        })\n",
        "\n",
        "    # 4. å•Ÿå‹•å¤šæ ¸é‹ç®—\n",
        "    cache = {}\n",
        "    print(f\"ğŸ”¥ å•Ÿå‹•ä¸¦è¡Œé‹ç®— (æ ¸å¿ƒæ•¸: {max_workers})ï¼Œå”¯ä¸€ä»»å‹™æ•¸: {len(task_list)}...\")\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        results = list(executor.map(_core_compute_engine, task_list))\n",
        "\n",
        "    for r in results:\n",
        "        cache[r['fingerprint']] = r['stats']\n",
        "\n",
        "    # 5. æ˜ å°„å›åŸå§‹å› å­åˆ—è¡¨\n",
        "    final_output = []\n",
        "    for f in factors:\n",
        "        f_hash = factor_to_hash[f]\n",
        "        stats = cache[f_hash]\n",
        "        score = (stats['e_m'] * w1) + (stats['e_v'] * w2) + (stats['cv'] * w3)\n",
        "        final_output.append({\n",
        "            'Factor': f,\n",
        "            'Unique_Cats': stats['u'],\n",
        "            'Eta_Mean': stats['e_m'],\n",
        "            'Eta_Var': stats['e_v'],\n",
        "            'Cramers_V': stats['cv'],\n",
        "            'Score': score\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(final_output).sort_values('Score', ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "PKiE5rhMd8Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. ç‰ˆæœ¬2"
      ],
      "metadata": {
        "id": "NrxrOnyrfTYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# --- æ ¸å¿ƒé‹ç®—å­ç¨‹å¼ (å¿…é ˆæ”¾åœ¨é ‚å±¤ä»¥ä¾¿ Pickle) ---\n",
        "\n",
        "def _worker_task(data_bundle):\n",
        "    \"\"\"\n",
        "    å–®ä¸€æ ¸å¿ƒåŸ·è¡Œçš„ä»»å‹™ï¼šè¨ˆç®— Eta_Mean, Eta_Var, Cramer's V\n",
        "    data_bundle åŒ…å«: (factor_series, y_series, y_dev_series, cluster_series, sst_mean, sst_var)\n",
        "    \"\"\"\n",
        "    f_ser, y_ser, y_dev_ser, c_ser, sst_m, sst_v = data_bundle\n",
        "\n",
        "    # é€™è£¡èª¿ç”¨ä¹‹å‰å®šç¾©å¥½çš„è¨ˆç®—é‚è¼¯ (ç°¡åŒ–ç‰ˆç¤ºæ„)\n",
        "    e_mean = _calculate_eta_sq_raw(f_ser, y_ser, sst_m)\n",
        "    e_var = _calculate_eta_sq_raw(f_ser, y_dev_ser, sst_v)\n",
        "    cv = _calculate_cramers_v_raw(f_ser, c_ser)\n",
        "\n",
        "    return {'e_m': e_mean, 'e_v': e_var, 'cv': cv, 'u': f_ser.nunique()}\n",
        "\n",
        "def _calculate_eta_sq_raw(f_ser, y_ser, sst):\n",
        "    if sst <= 0 or f_ser.nunique() < 2: return 0.0\n",
        "    f_ser_filled = f_ser.fillna('NA')\n",
        "    grand_mean = y_ser.mean()\n",
        "    group_means = y_ser.groupby(f_ser_filled).mean()\n",
        "    group_counts = y_ser.groupby(f_ser_filled).count()\n",
        "    ssb = np.sum(group_counts * (group_means - grand_mean)**2)\n",
        "    return min(ssb / sst, 1.0)\n",
        "\n",
        "def _calculate_cramers_v_raw(f_ser, c_ser):\n",
        "    confusion_matrix = pd.crosstab(f_ser.fillna('NA'), c_ser)\n",
        "    if confusion_matrix.empty or min(confusion_matrix.shape) < 2: return 0.0\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = len(f_ser)\n",
        "    phi2 = max(0, (chi2 / n) - ((confusion_matrix.shape[1]-1)*(confusion_matrix.shape[0]-1))/(n-1))\n",
        "    r_corr = confusion_matrix.shape[0] - ((confusion_matrix.shape[0]-1)**2)/(n-1)\n",
        "    k_corr = confusion_matrix.shape[1] - ((confusion_matrix.shape[1]-1)**2)/(n-1)\n",
        "    denom = min((k_corr-1), (r_corr-1))\n",
        "    return np.sqrt(phi2 / denom) if denom > 0 else 0\n",
        "\n",
        "# --- ä¸»åˆ†æç¨‹å¼ ---\n",
        "\n",
        "def analyze_machine_impact_multiprocess(df, setting_dict, max_workers=None):\n",
        "    v_col = setting_dict['value_colname']\n",
        "    c_col = setting_dict['cluster_colname']\n",
        "    factors = setting_dict['factor_col_list']\n",
        "    w = setting_dict['proportion']\n",
        "\n",
        "    # 1. é è™•ç†\n",
        "    y_ser = df[v_col]\n",
        "    y_dev_ser = np.abs(y_ser - y_ser.median())\n",
        "    c_ser = df[c_col]\n",
        "    sst_m = np.sum((y_ser - y_ser.mean())**2)\n",
        "    sst_v = np.sum((y_dev_ser - y_dev_ser.mean())**2)\n",
        "\n",
        "    # 2. æŒ‡ç´‹è¾¨è­˜ï¼šæ‰¾å‡ºå”¯ä¸€å…§å®¹çš„æ¬„ä½\n",
        "    unique_patterns = {} # {hash: first_column_name}\n",
        "    col_to_hash = {}    # {column_name: hash}\n",
        "\n",
        "    for col in factors:\n",
        "        # ä½¿ç”¨ tobytes() å¿«é€Ÿç”ŸæˆæŒ‡ç´‹\n",
        "        h = hashlib.md5(df[col].fillna('NA').astype(str).values.tobytes()).hexdigest()\n",
        "        col_to_hash[col] = h\n",
        "        if h not in unique_patterns:\n",
        "            unique_patterns[h] = col\n",
        "\n",
        "    # 3. æº–å‚™ä¸¦è¡Œä»»å‹™ (åªé‡å°å”¯ä¸€çš„ Hash)\n",
        "    unique_hashes = list(unique_patterns.keys())\n",
        "    tasks = [\n",
        "        (df[unique_patterns[h]], y_ser, y_dev_ser, c_ser, sst_m, sst_v)\n",
        "        for h in unique_hashes\n",
        "    ]\n",
        "\n",
        "    # 4. å¤šæ ¸æ´¾ç™¼\n",
        "    cache = {}\n",
        "    print(f\"ğŸŒ€ åŸå§‹æ¬„ä½: {len(factors)}, å”¯ä¸€å…§å®¹: {len(unique_hashes)}ã€‚ä½¿ç”¨å¤šæ ¸è¨ˆç®—ä¸­...\")\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        task_results = list(executor.map(_worker_task, tasks))\n",
        "\n",
        "    for h, res in zip(unique_hashes, task_results):\n",
        "        cache[h] = res\n",
        "\n",
        "    # 5. æ˜ å°„çµæœä¸¦è¨ˆç®—ç¸½åˆ†\n",
        "    final_results = []\n",
        "    for col in factors:\n",
        "        h = col_to_hash[col]\n",
        "        stats = cache[h]\n",
        "        score = (stats['e_m'] * w[0]) + (stats['e_v'] * w[1]) + (stats['cv'] * w[2])\n",
        "        final_results.append({\n",
        "            'Factor': col, 'Score': score,\n",
        "            'Eta_Mean': stats['e_m'], 'Eta_Var': stats['e_v'], 'Cramers_V': stats['cv'],\n",
        "            'Unique_Cats': stats['u']\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(final_results).sort_values('Score', ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "hY9zbe8IfV33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **ä¸¦è¡Œé‹ç®—çš„æ³¨æ„äº‹é …**\n",
        "1. Pickle é™åˆ¶ï¼šmultiprocessing éœ€è¦å°‡è³‡æ–™åºåˆ—åŒ–ï¼ˆPickleï¼‰å‚³é€çµ¦å­é€²ç¨‹ã€‚å› æ­¤ï¼Œæ‰€æœ‰å­ç¨‹å¼ï¼ˆå¦‚ _worker_taskï¼‰å¿…é ˆå®šç¾©åœ¨ Python æª”æ¡ˆçš„æœ€é ‚å±¤ï¼Œä¸èƒ½å®šç¾©åœ¨å¦ä¸€å€‹å‡½æ•¸å…§éƒ¨ã€‚\n",
        "\n",
        "2. è³‡æ–™è¤‡è£½æˆæœ¬ï¼šå¤šé€²ç¨‹æœƒè¤‡è£½ä¸€ä»½ DataFrame åˆ°å­é€²ç¨‹ç©ºé–“ã€‚å¦‚æœä½ çš„ DataFrame é«˜é”æ•¸ GBï¼Œå»ºè­°åªå‚³éå¿…è¦çš„ Series è€Œä¸æ˜¯æ•´å€‹ DataFrameï¼Œå¦‚æˆ‘åœ¨ç¯„ä¾‹ä¸­æ‰€åšçš„é‚£æ¨£ã€‚"
      ],
      "metadata": {
        "id": "kcuSE9KkffEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "9QD7TwpEfyWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’¡**ç¸½çµ**"
      ],
      "metadata": {
        "id": "uQxfLBSrfzpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "é€™ä»½åˆ†ææ–¹æ¡ˆæ˜¯å°ˆç‚ºåŠå°é«”æˆ–è£½é€ æ¥­çš„**æ ¹å› åˆ†æ (Root Cause Analysis, RCA)** é‡èº«æ‰“é€ çš„ã€‚å®ƒçµåˆäº†çµ±è¨ˆå­¸çš„åš´è¬¹æ€§èˆ‡è³‡æ–™å·¥ç¨‹çš„æ•ˆèƒ½å„ªåŒ–ï¼Œæ—¨åœ¨å¾æˆåƒä¸Šè¬å€‹æ©Ÿå°æ¬„ä½ä¸­ï¼Œç²¾æº–æŠ“å‡ºå°è‡´ç”¢å“å“è³ªåç§»çš„ã€Œå«Œç–‘çŠ¯ã€ã€‚\n",
        "\n",
        "ä»¥ä¸‹æ˜¯è©²æ–¹æ¡ˆçš„å››å€‹æ ¸å¿ƒé‡é»ç¸½çµï¼š\n",
        "\n",
        "---\n",
        "\n",
        "## 1. å»ºç«‹ã€Œä¸‰åˆä¸€ã€ç¶œåˆè©•ä¼°æŒ‡æ¨™\n",
        "\n",
        "ç‚ºäº†åŒæ™‚æ•æ‰ã€Œæ•¸å€¼åç§»ã€èˆ‡ã€Œç©©å®šåº¦è®ŠåŒ–ã€ï¼Œæˆ‘å€‘ä¸åªçœ‹å¹³å‡å€¼ï¼Œè€Œæ˜¯å»ºç«‹äº†ä¸€å€‹å¤šç¶­åº¦çš„è©•åˆ†ç³»çµ±ï¼š\n",
        "\n",
        "* **Eta-squared ()**ï¼šé‡åŒ–æ©Ÿå°å° **y å€¼å¹³å‡åç§»**çš„è§£é‡‹åŠ›ï¼ˆä¾‹å¦‚ï¼šæ©Ÿå° A è®“é˜»å€¼é›†é«”ä¸Šå‡ï¼‰ã€‚\n",
        "* **Eta-squared ()**ï¼šé‡åŒ–æ©Ÿå°å° **y å€¼æ³¢å‹•ç¨‹åº¦**çš„è§£é‡‹åŠ›ï¼ˆä¾‹å¦‚ï¼šæ©Ÿå° B è®“ç”¢å“å“è³ªè®Šå¾—å¾ˆä¸ç©©å®šï¼‰ã€‚\n",
        "* **Cramer's V**ï¼šè¡¡é‡æ©Ÿå°é¡åˆ¥èˆ‡ **VBGMM åˆ†ç¾¤æ¨™ç±¤**çš„ä¸€è‡´æ€§ï¼ˆæª¢è¦–æ©Ÿå°åˆ†çµ„æ˜¯å¦èƒ½å®Œç¾é æ¸¬ Good/Badï¼‰ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## 2. é‡å°ç”Ÿç”¢è³‡æ–™çš„å¼·å¥æ€§è™•ç†\n",
        "\n",
        "ç”Ÿç”¢ç¾å ´çš„è³‡æ–™å¾€å¾€ä¸å®Œç¾ï¼Œæˆ‘å€‘åœ¨è¨ˆç®—æµç¨‹ä¸­åŠ å…¥é˜²ç¦¦æ€§æ©Ÿåˆ¶ï¼š\n",
        "\n",
        "* **ç¼ºå¤±å€¼æ¨™è¨˜**ï¼šå°‡æ©Ÿå°æ¬„ä½çš„ç¼ºå¤±ï¼ˆNaNï¼‰è¦–ç‚ºç¨ç«‹é¡åˆ¥ `'NA'`ï¼Œå› ç‚ºã€Œç¼ºè€ƒã€æœ¬èº«å¯èƒ½å°±æ˜¯ä¸€ç¨®è£½ç¨‹ç•°å¸¸è¨Šè™Ÿã€‚\n",
        "* **ç•°å¸¸éæ¿¾**ï¼šè‡ªå‹•è­˜åˆ¥ä¸¦è·³éã€Œå–®ä¸€åˆ†é¡ã€æˆ–ã€Œç¸½è®Šç•°ç‚ºé›¶ã€çš„ç„¡æ•ˆæ¬„ä½ï¼Œé¿å…ç¨‹å¼ç™¼ç”Ÿé™¤ä»¥é›¶çš„éŒ¯èª¤ã€‚\n",
        "* **é ç®—ç¸½å¹³æ–¹å’Œ (SST)**ï¼šåœ¨é€²å…¥è¿´åœˆå‰å…ˆè¨ˆç®—å…¨åŸŸè®Šç•°ï¼Œé¡¯è‘—æå‡é‹ç®—æ•ˆç‡ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## 3. æŒ‡ç´‹è¾¨è­˜èˆ‡å¿«å–æŠ€è¡“ (Hashing & Caching)\n",
        "\n",
        "ç•¶æ©Ÿå°å› å­é«˜é”æ•¸åƒå€‹æ™‚ï¼Œå…§å®¹é‡è¤‡ï¼ˆå…§å®¹ç›¸åŒä½†åç¨±ä¸åŒï¼‰æ˜¯æ•ˆèƒ½æ®ºæ‰‹ï¼š\n",
        "\n",
        "* **æŒ‡ç´‹è­˜åˆ¥ (Hashing)**ï¼šåˆ©ç”¨ MD5 æ¼”ç®—æ³•å°‡æ¬„ä½å…§å®¹è½‰åŒ–ç‚ºå›ºå®šé•·åº¦çš„æŒ‡ç´‹ã€‚åªè¦å…©å°æ©Ÿå°çš„ç”Ÿç”¢åºåˆ—å®Œå…¨ç›¸åŒï¼ŒæŒ‡ç´‹å°±æœƒä¸€è‡´ã€‚\n",
        "* **çµæœå¿«å– (Caching)**ï¼šåŒæŒ‡ç´‹çš„æ©Ÿå°åªè¨ˆç®—ä¸€æ¬¡çµ±è¨ˆæŒ‡æ¨™ã€‚è‹¥æœ‰ 2000 å€‹æ¬„ä½ä½†åªæœ‰ 100 ç¨®çµ„åˆï¼Œè¨ˆç®—é‡ç›´æ¥é™ä½ **95%**ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## 4. å¤šæ ¸ä¸¦è¡Œé‹ç®—å„ªåŒ–\n",
        "\n",
        "ç‚ºäº†å£“æ¦¨ç¡¬é«”æ•ˆèƒ½ï¼Œæˆ‘å€‘æ¡ç”¨ã€Œå…ˆå»é‡ã€å†ä¸¦è¡Œã€çš„ç­–ç•¥ï¼š\n",
        "\n",
        "* **ä»»å‹™æ´¾ç™¼**ï¼šåªå°‡æŒ‡ç´‹ä¸é‡è¤‡çš„æ¬„ä½æ´¾ç™¼çµ¦ `ProcessPoolExecutor`ã€‚\n",
        "* **è·¨æ ¸åŠ é€Ÿ**ï¼šå¤šå€‹ CPU æ ¸å¿ƒåŒæ­¥é€²è¡Œçµ±è¨ˆé‹ç®—ï¼Œè®“è™•ç†æ•¸åƒå€‹æ¬„ä½çš„åˆ†ææ™‚é–“ç¸®çŸ­è‡³ç§’ç´šã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ’¡ æœ€çµ‚å»ºè­°ï¼šå¦‚ä½•è§£è®€åˆ†æå ±å‘Šï¼Ÿ\n",
        "\n",
        "ç•¶ä½ å¾—åˆ°æœ€çµ‚çš„ Rank æ’åè¡¨æ™‚ï¼š\n",
        "\n",
        "1. **é«˜ Score + é«˜ Eta_Mean**ï¼šå„ªå…ˆæª¢æŸ¥æ©Ÿå°çš„**åƒæ•¸è¨­å®š**ï¼ˆä¾‹å¦‚æº«åº¦ã€å£“åŠ›æ˜¯å¦åé›¢ä¸­å¿ƒå€¼ï¼‰ã€‚\n",
        "2. **é«˜ Score + é«˜ Eta_Var**ï¼šå„ªå…ˆæª¢æŸ¥æ©Ÿå°çš„**é›¶ä»¶è€åŒ–æˆ–ç©©å®šæ€§**ï¼ˆä¾‹å¦‚æ„Ÿæ¸¬å™¨æ¼‚ç§»ã€é›¶ä»¶ç£¨æï¼‰ã€‚\n",
        "3. **Unique_Cats æ•¸é‡**ï¼šç•™æ„é¡åˆ¥æ•¸éå¤šçš„æ¬„ä½ï¼ˆå¦‚æµæ°´è™Ÿï¼‰ï¼Œè‹¥åˆ†æ•¸é«˜ä½†é¡åˆ¥æ•¸æ¥µå¤šï¼Œéœ€è­¦æƒ•éæ“¬åˆé¢¨éšªã€‚\n",
        "\n",
        "**é€™å¥—å·¥å…·å·²ç¶“æº–å‚™å¥½æ‡‰å°å¤§è¦æ¨¡çš„ RCA ä»»å‹™ã€‚å¦‚æœä½ éœ€è¦æ›´é€²ä¸€æ­¥å°‡çµæœè¦–è¦ºåŒ–ï¼ˆå¦‚ç†±é»åœ–æˆ–é›·é”åœ–ï¼‰ï¼Œæˆ‘ä¹Ÿèƒ½ç‚ºä½ æä¾›ç›¸é—œä»£ç¢¼å»ºè­°ã€‚**"
      ],
      "metadata": {
        "id": "gSgziPJyfuR9"
      }
    }
  ]
}